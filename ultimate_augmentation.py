#!/usr/bin/env python3
"""
Ultimate æ•°æ®å¢å¼ºæ¨¡å—

æ•´åˆ:
1. å¢å¼ºç‰ˆæ¡ä»¶æ‰©æ•£æ¨¡å‹
2. è¾¹é¢„æµ‹å™¨å›¾æ„å»º
3. è‡ªé€‚åº”è´¨é‡æ§åˆ¶
4. å¤šæ ·æ€§ä¿è¯
"""
import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from torch_geometric.data import Data

# å¯¼å…¥æˆ‘ä»¬åˆ›å»ºçš„å¢å¼ºç‰ˆæ¨¡å‹
try:
    from enhanced_diffusion_model import EnhancedConditionalDiffusionModel
    ENHANCED_AVAILABLE = True
except ImportError:
    print("âš ï¸  å¢å¼ºç‰ˆæ‰©æ•£æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†ç‰ˆæœ¬")
    from ddpm_diffusion_model import EnhancedDiffusionModel
    ENHANCED_AVAILABLE = False

from edge_predictor_augmentation import ImprovedEdgePredictor, build_edges_with_edge_predictor


def ultimate_augment_protein(
    protein_data,
    diffusion_model,
    edge_predictor,
    config,
    target_ratio=0.9
):
    """
    ç»ˆææ•°æ®å¢å¼º - å•ä¸ªè›‹ç™½è´¨
    
    Args:
        protein_data: PyG Data object
        diffusion_model: è®­ç»ƒå¥½çš„æ‰©æ•£æ¨¡å‹
        edge_predictor: è®­ç»ƒå¥½çš„è¾¹é¢„æµ‹å™¨
        config: UltimateConfigå¯¹è±¡
        target_ratio: ç›®æ ‡æ­£æ ·æœ¬æ¯”ä¾‹
        
    Returns:
        augmented_data: å¢å¼ºåçš„Dataå¯¹è±¡
        stats: ç»Ÿè®¡ä¿¡æ¯
    """
    # è®¡ç®—éœ€è¦ç”Ÿæˆçš„æ•°é‡
    n_pos = (protein_data.y == 1).sum().item()
    n_neg = (protein_data.y == 0).sum().item()
    total_nodes = n_pos + n_neg
    
    target_pos = int(total_nodes * target_ratio)
    n_to_generate_base = max(config.min_samples_per_protein, target_pos - n_pos)
    
    # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šç”Ÿæˆå¤šå€å€™é€‰æ ·æœ¬
    sample_multiplier = config.enhanced_diffusion_config.get('sample_multiplier', 10)
    n_to_generate = n_to_generate_base * sample_multiplier
    
    if n_to_generate <= 0:
        return protein_data, {'num_generated': 0, 'quality': 0, 'diversity': 0}
    
    # ğŸš€ GPUåŠ é€Ÿï¼šæ‰€æœ‰æ“ä½œä¿æŒåœ¨GPUä¸Š
    device = config.device

    # ç”Ÿæˆæ ·æœ¬
    if ENHANCED_AVAILABLE and config.use_enhanced_diffusion:
        # ä½¿ç”¨å¢å¼ºç‰ˆæ¡ä»¶æ‰©æ•£ï¼ˆè¿”å›GPUå¼ é‡ï¼‰
        generated_samples, quality_scores = diffusion_model.generate_positive_sample(
            protein_data,
            num_samples=n_to_generate,
            quality_threshold=config.enhanced_diffusion_config['quality_threshold'],
            max_attempts=config.enhanced_diffusion_config['max_attempts']
        )
        # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        generated_samples = generated_samples.to(device)
        quality_scores = quality_scores.to(device)
    else:
        # ä½¿ç”¨æ ‡å‡†æ‰©æ•£æ¨¡å‹
        generated_samples = diffusion_model.generate_positive_sample(
            protein_data.protein_context,
            num_samples=n_to_generate
        )
        if generated_samples is None or len(generated_samples) == 0:
            return protein_data, {'num_generated': 0, 'quality': 0, 'diversity': 0}

        # è¯„ä¼°è´¨é‡ï¼ˆGPUç‰ˆæœ¬ï¼‰
        generated_samples_tensor = torch.tensor(generated_samples, device=device, dtype=torch.float32)
        positive_samples = protein_data.x[protein_data.y == 1].to(device) if n_pos > 0 else protein_data.x.to(device)
        quality_scores = evaluate_sample_quality_gpu(generated_samples_tensor, positive_samples, config.quality_threshold)
        generated_samples = generated_samples_tensor

    # ğŸš€ è´¨é‡è¿‡æ»¤ï¼ˆGPUæ“ä½œï¼‰
    quality_mask = quality_scores > config.quality_threshold
    if quality_mask.sum() == 0:
        # å¦‚æœæ²¡æœ‰é«˜è´¨é‡æ ·æœ¬ï¼Œé™ä½é˜ˆå€¼
        quality_mask = quality_scores > (config.quality_threshold * 0.7)

    filtered_samples = generated_samples[quality_mask]
    filtered_scores = quality_scores[quality_mask]

    # ğŸš€ å¦‚æœè¿‡æ»¤åæ ·æœ¬å¤ªå°‘ï¼Œé€‰æ‹©top-kï¼ˆGPUæ“ä½œï¼‰
    if filtered_samples.size(0) < n_to_generate_base:
        top_k_indices = torch.topk(quality_scores, k=min(n_to_generate_base, quality_scores.size(0))).indices
        filtered_samples = generated_samples[top_k_indices]
        filtered_scores = quality_scores[top_k_indices]
    elif filtered_samples.size(0) > n_to_generate_base * 2:
        # å¦‚æœæ ·æœ¬å¤ªå¤šï¼Œé€‰æ‹©æœ€ä¼˜çš„ï¼ˆGPUæ“ä½œï¼‰
        top_k_indices = torch.topk(filtered_scores, k=n_to_generate_base).indices
        filtered_samples = filtered_samples[top_k_indices]
        filtered_scores = filtered_scores[top_k_indices]

    # å·²ç»æ˜¯torch tensorï¼Œä¿æŒåœ¨GPUä¸Š
    generated_x = filtered_samples
    generated_y = torch.ones(generated_x.size(0), dtype=torch.long, device=device)
    
    # ä½¿ç”¨è¾¹é¢„æµ‹å™¨æ„å»ºå›¾
    augmented_graph, edge_stats = build_edges_with_edge_predictor(
        protein_data,
        generated_x,
        edge_predictor,
        config.device,
        **config.edge_predictor_config
    )
    
    # ğŸš€ è®¡ç®—å¤šæ ·æ€§ï¼ˆGPUæ“ä½œï¼‰
    if filtered_samples.size(0) > 1:
        diversity = calculate_diversity_gpu(filtered_samples)
    else:
        diversity = 0.5

    stats = {
        'num_generated': filtered_samples.size(0),
        'num_candidates': generated_samples.size(0),
        'quality': float(filtered_scores.mean().item()),  # GPU â†’ scalar
        'diversity': diversity,
        'num_edges': edge_stats.get('total_edges', 0),
        'avg_edge_score': edge_stats.get('avg_edge_score', 0)
    }
    
    return augmented_graph, stats


def ultimate_augment_dataset(
    dataset,
    diffusion_model,
    edge_predictor,
    config
):
    """
    ç»ˆææ•°æ®å¢å¼º - æ•´ä¸ªæ•°æ®é›†
    
    Returns:
        augmented_dataset: å¢å¼ºåçš„æ•°æ®é›†
        global_stats: å…¨å±€ç»Ÿè®¡ä¿¡æ¯
    """
    print(f"\nğŸš€ ULTIMATE æ•°æ®å¢å¼ºå¼€å§‹")
    print(f"  ç­–ç•¥: å¢å¼ºç‰ˆæ‰©æ•£ + è¾¹é¢„æµ‹å™¨ + è´¨é‡æ§åˆ¶")
    print(f"  ç›®æ ‡æ¯”ä¾‹: {config.target_ratio:.1%}")
    print(f"  è´¨é‡é˜ˆå€¼: {config.quality_threshold}")
    print(f"  é‡‡æ ·å€æ•°: {config.enhanced_diffusion_config.get('sample_multiplier', 10)}x")
    
    augmented_dataset = []
    all_stats = []
    
    for protein_data in tqdm(dataset, desc="Ultimate augmenting"):
        try:
            aug_data, stats = ultimate_augment_protein(
                protein_data,
                diffusion_model,
                edge_predictor,
                config,
                target_ratio=config.target_ratio
            )
            
            augmented_dataset.append(aug_data)
            all_stats.append(stats)
            
        except Exception as e:
            print(f"âš ï¸  å¢å¼ºå¤±è´¥ ({protein_data.name if hasattr(protein_data, 'name') else 'unknown'}): {e}")
            augmented_dataset.append(protein_data)
            all_stats.append({'num_generated': 0, 'quality': 0, 'diversity': 0})
    
    # å…¨å±€ç»Ÿè®¡
    total_generated = sum(s['num_generated'] for s in all_stats)
    avg_quality = np.mean([s['quality'] for s in all_stats if s['quality'] > 0])
    avg_diversity = np.mean([s['diversity'] for s in all_stats if s['diversity'] > 0])
    total_edges = sum(s.get('num_edges', 0) for s in all_stats)
    
    global_stats = {
        'total_proteins': len(dataset),
        'total_generated': total_generated,
        'avg_quality': avg_quality,
        'avg_diversity': avg_diversity,
        'total_edges': total_edges,
        'success_rate': sum(1 for s in all_stats if s['num_generated'] > 0) / len(all_stats)
    }
    
    print(f"\nâœ… ULTIMATE å¢å¼ºå®Œæˆ:")
    print(f"  æ€»è›‹ç™½è´¨æ•°: {global_stats['total_proteins']}")
    print(f"  ç”Ÿæˆæ ·æœ¬æ•°: {global_stats['total_generated']:,}")
    print(f"  å¹³å‡è´¨é‡: {global_stats['avg_quality']:.3f}")
    print(f"  å¹³å‡å¤šæ ·æ€§: {global_stats['avg_diversity']:.3f}")
    print(f"  æˆåŠŸç‡: {global_stats['success_rate']:.1%}")
    print(f"  æ€»è¾¹æ•°: {global_stats['total_edges']:,}")
    
    return augmented_dataset, global_stats


def evaluate_sample_quality(generated_samples, positive_samples, threshold=0.5):
    """è¯„ä¼°ç”Ÿæˆæ ·æœ¬è´¨é‡ï¼ˆç”¨äºæ ‡å‡†æ‰©æ•£æ¨¡å‹ï¼ŒNumPyç‰ˆæœ¬ï¼‰"""
    if len(positive_samples) == 0:
        return np.ones(len(generated_samples)) * 0.5

    pos_mean = np.mean(positive_samples, axis=0)
    pos_std = np.std(positive_samples, axis=0) + 1e-6

    # å½’ä¸€åŒ–è·ç¦»
    normalized = (generated_samples - pos_mean) / pos_std
    dist = np.mean(normalized ** 2, axis=1)

    # è½¬æ¢ä¸ºè´¨é‡åˆ†æ•°
    quality = 1.0 / (1.0 + dist)

    return quality


def evaluate_sample_quality_gpu(generated_samples, positive_samples, threshold=0.5):
    """ğŸš€ è¯„ä¼°ç”Ÿæˆæ ·æœ¬è´¨é‡ï¼ˆGPUåŠ é€Ÿç‰ˆæœ¬ï¼‰"""
    if positive_samples.size(0) == 0:
        return torch.ones(generated_samples.size(0), device=generated_samples.device) * 0.5

    pos_mean = torch.mean(positive_samples, dim=0)
    pos_std = torch.std(positive_samples, dim=0) + 1e-6

    # å½’ä¸€åŒ–è·ç¦»ï¼ˆGPUæ“ä½œï¼‰
    normalized = (generated_samples - pos_mean) / pos_std
    dist = torch.mean(normalized ** 2, dim=1)

    # è½¬æ¢ä¸ºè´¨é‡åˆ†æ•°
    quality = 1.0 / (1.0 + dist)

    return quality


def calculate_diversity(samples):
    """è®¡ç®—æ ·æœ¬å¤šæ ·æ€§ï¼ˆNumPyç‰ˆæœ¬ï¼‰"""
    if len(samples) < 2:
        return 0.5

    # è®¡ç®—æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦
    samples_norm = samples / (np.linalg.norm(samples, axis=1, keepdims=True) + 1e-8)
    similarity_matrix = np.dot(samples_norm, samples_norm.T)

    # å»é™¤å¯¹è§’çº¿ï¼ˆè‡ªèº«ç›¸ä¼¼åº¦ï¼‰
    mask = np.ones_like(similarity_matrix, dtype=bool)
    np.fill_diagonal(mask, False)

    # å¤šæ ·æ€§ = 1 - å¹³å‡ç›¸ä¼¼åº¦
    avg_similarity = np.mean(similarity_matrix[mask])
    diversity = 1.0 - avg_similarity

    return max(0.0, min(1.0, diversity))


def calculate_diversity_gpu(samples):
    """ğŸš€ è®¡ç®—æ ·æœ¬å¤šæ ·æ€§ï¼ˆGPUåŠ é€Ÿç‰ˆæœ¬ï¼‰"""
    if samples.size(0) < 2:
        return 0.5

    # è®¡ç®—æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆGPUæ“ä½œï¼‰
    samples_norm = F.normalize(samples, p=2, dim=1)  # L2å½’ä¸€åŒ–
    similarity_matrix = torch.mm(samples_norm, samples_norm.t())  # ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ

    # å»é™¤å¯¹è§’çº¿ï¼ˆè‡ªèº«ç›¸ä¼¼åº¦ï¼‰
    mask = torch.ones_like(similarity_matrix, dtype=torch.bool)
    mask.fill_diagonal_(False)

    # å¤šæ ·æ€§ = 1 - å¹³å‡ç›¸ä¼¼åº¦
    avg_similarity = similarity_matrix[mask].mean().item()
    diversity = 1.0 - avg_similarity

    return max(0.0, min(1.0, diversity))
