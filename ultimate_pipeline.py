#!/usr/bin/env python3
"""
ğŸš€ ULTIMATE PIPELINE for DNA Binding Site Prediction

æ•´åˆæ‰€æœ‰ä¼˜åŒ–:
1. å¢å¼ºç‰ˆæ¡ä»¶æ‰©æ•£æ¨¡å‹ (EnhancedConditionalDiffusionModel)
2. é«˜çº§GAT-GNNæ¨¡å‹ (AdvancedBindingSiteGNN)  
3. PPIè¾¹é¢„æµ‹å™¨
4. è‡ªé€‚åº”æ•°æ®å¢å¼º
5. äº¤å‰éªŒè¯ + é›†æˆå­¦ä¹ 

é¢„æœŸæ€§èƒ½æå‡:
- æ•°æ®è´¨é‡: 0.178 â†’ 0.65+
- æ•°æ®æ¯”ä¾‹: 22% â†’ 90%
- F1 Score: 0.48 â†’ 0.60+
- MCC: 0.42 â†’ 0.55+
"""
import os
import sys
import time
import glob
import json
import warnings
import numpy as np
import torch
from tqdm import tqdm
from sklearn.model_selection import KFold
from torch_geometric.data import Data

warnings.filterwarnings('ignore')

# å¯¼å…¥é…ç½®
from ultimate_config import UltimateConfig
from data_loader import ProteinDataset
from main import calculate_class_ratio,set_seed

# å¯¼å…¥å¢å¼ºç‰ˆæ¨¡å‹
try:
    from enhanced_diffusion_model import EnhancedConditionalDiffusionModel
    ENHANCED_DIFFUSION_AVAILABLE = True
    print("âœ… å¢å¼ºç‰ˆæ‰©æ•£æ¨¡å‹å·²åŠ è½½")
except ImportError as e:
    print(f"âš ï¸  å¢å¼ºç‰ˆæ‰©æ•£æ¨¡å‹ä¸å¯ç”¨: {e}")
    print("   ä½¿ç”¨æ ‡å‡†æ‰©æ•£æ¨¡å‹")
    from ddpm_diffusion_model import EnhancedDiffusionModel
    ENHANCED_DIFFUSION_AVAILABLE = False

try:
    from advanced_gnn_model import AdvancedBindingSiteGNN
    ADVANCED_GNN_AVAILABLE = True
    print("âœ… é«˜çº§GNNæ¨¡å‹å·²åŠ è½½")
except ImportError as e:
    print(f"âš ï¸  é«˜çº§GNNæ¨¡å‹ä¸å¯ç”¨: {e}")
    print("   ä½¿ç”¨æ ‡å‡†GNNæ¨¡å‹")
    from improved_gnn_model import ImprovedBindingSiteGNN
    ADVANCED_GNN_AVAILABLE = False

# å¯¼å…¥å¢å¼ºæ¨¡å—
from ultimate_augmentation import ultimate_augment_dataset
from edge_predictor_augmentation import ImprovedEdgePredictor


def load_dataset_quiet(dataset_file, config):
    """é™é»˜åŠ è½½æ•°æ®é›†"""
    import shutil
    import contextlib

    temp_data_dir = os.path.join(config.data_dir, "temp_ultimate")
    os.makedirs(temp_data_dir, exist_ok=True)

    temp_file = os.path.join(temp_data_dir, os.path.basename(dataset_file))
    shutil.copy2(dataset_file, temp_file)

    try:
        if not config.verbose_loading:
            with open(os.devnull, 'w') as devnull:
                with contextlib.redirect_stdout(devnull):
                    dataset_loader = ProteinDataset(temp_data_dir, device=config.device)
                    dataset = dataset_loader.proteins
        else:
            dataset_loader = ProteinDataset(temp_data_dir, device=config.device)
            dataset = dataset_loader.proteins
    finally:
        if os.path.exists(temp_data_dir):
            shutil.rmtree(temp_data_dir)

    return dataset


def load_edge_predictor(config):
    """åŠ è½½è¾¹é¢„æµ‹å™¨"""
    print(f"\nğŸ”— åŠ è½½è¾¹é¢„æµ‹å™¨ (è¶…ç¨³å®šç‰ˆ)...")
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    ppi_model_path = os.path.join(current_dir, "models", "edge_predictor_best_ultra_stable.pth")
    
    if not os.path.exists(ppi_model_path):
        print(f"âŒ è¾¹é¢„æµ‹å™¨æ¨¡å‹ä¸å­˜åœ¨: {ppi_model_path}")
        return None
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(ppi_model_path, map_location='cpu', weights_only=False)
    
    # æ¨æ–­hidden_dim
    if 'model_state_dict' in checkpoint:
        hidden_dim = checkpoint['model_state_dict']['fc_transform.weight'].shape[0]
    else:
        hidden_dim = checkpoint['fc_transform.weight'].shape[0]
    
    # åˆ›å»ºè¾¹é¢„æµ‹å™¨
    edge_predictor = ImprovedEdgePredictor(
        input_dim=1280,  # ESM2ç‰¹å¾ç»´åº¦
        hidden_dim=hidden_dim
    )
    
    # åŠ è½½æƒé‡
    if 'model_state_dict' in checkpoint:
        edge_predictor.load_state_dict(checkpoint['model_state_dict'])
    else:
        edge_predictor.load_state_dict(checkpoint)
    
    edge_predictor.to(config.device)
    edge_predictor.eval()
    
    print(f"âœ… è¾¹é¢„æµ‹å™¨å·²åŠ è½½ (hidden_dim={hidden_dim})")
    print(f"   è®­ç»ƒAUC: {checkpoint.get('best_auc', 'N/A')}")
    
    return edge_predictor


def train_enhanced_diffusion_model(train_dataset, config):
    """è®­ç»ƒå¢å¼ºç‰ˆæ‰©æ•£æ¨¡å‹"""
    print(f"\nğŸ§  è®­ç»ƒå¢å¼ºç‰ˆæ‰©æ•£æ¨¡å‹...")
    
    if ENHANCED_DIFFUSION_AVAILABLE and config.use_enhanced_diffusion:
        # ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹
        diffusion_model = EnhancedConditionalDiffusionModel(
            input_dim=config.diffusion_input_dim,
            T=config.enhanced_diffusion_config['T'],
            hidden_dim=config.enhanced_diffusion_config['hidden_dim'],
            context_dim=config.enhanced_diffusion_config['context_dim'],
            device=config.device
        )
        
        print(f"  æ¨¡å‹ç±»å‹: æ¡ä»¶æ‰©æ•£ (Conditional DDPM)")
        print(f"  T={config.enhanced_diffusion_config['T']}, context_dim={config.enhanced_diffusion_config['context_dim']}")
    else:
        # ä½¿ç”¨æ ‡å‡†æ‰©æ•£æ¨¡å‹
        from ddpm_diffusion_model import EnhancedDiffusionModel
        diffusion_model = EnhancedDiffusionModel(
            input_dim=config.diffusion_input_dim,
            T=config.diffusion_T,
            device=config.device
        )
        print(f"  æ¨¡å‹ç±»å‹: æ ‡å‡†æ‰©æ•£ (DDPM)")
    
    # è®­ç»ƒ
    start_time = time.time()
    diffusion_model.train_on_positive_samples(
        train_dataset,
        epochs=config.diffusion_epochs,
        batch_size=config.diffusion_batch_size,
        lr=config.diffusion_lr
    )
    train_time = time.time() - start_time
    
    print(f"âœ… æ‰©æ•£æ¨¡å‹è®­ç»ƒå®Œæˆ: {train_time:.1f}ç§’")
    
    return diffusion_model


def create_gnn_model(config):
    """åˆ›å»ºGNNæ¨¡å‹"""
    if ADVANCED_GNN_AVAILABLE and config.use_advanced_gnn:
        # ä½¿ç”¨é«˜çº§GAT-GNN
        model = AdvancedBindingSiteGNN(
            input_dim=config.diffusion_input_dim,
            hidden_dim=config.advanced_gnn_config['hidden_dim'],
            num_layers=config.advanced_gnn_config['num_layers'],
            heads=config.advanced_gnn_config['heads'],
            dropout=config.advanced_gnn_config['dropout'],
            use_edge_features=config.advanced_gnn_config['use_edge_features'],
            focal_alpha=config.advanced_gnn_config['focal_alpha'],
            focal_gamma=config.advanced_gnn_config['focal_gamma'],
            class_balanced=config.advanced_gnn_config['class_balanced']
        )
        print(f"  æ¨¡å‹ç±»å‹: Advanced GAT-GNN")
        print(f"  å±‚æ•°={config.advanced_gnn_config['num_layers']}, å¤´æ•°={config.advanced_gnn_config['heads']}")
    else:
        # ä½¿ç”¨æ ‡å‡†GNN
        from improved_gnn_model import ImprovedBindingSiteGNN
        model = ImprovedBindingSiteGNN(
            input_dim=config.diffusion_input_dim,
            hidden_dim=config.gnn_hidden_dim,
            dropout=config.gnn_dropout
        )
        print(f"  æ¨¡å‹ç±»å‹: æ ‡å‡†GNN")
    
    return model


def cross_validation_training(augmented_data, original_data, config):
    """äº¤å‰éªŒè¯è®­ç»ƒ"""
    print(f"\nğŸ”„ {config.cv_folds}æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ...")
    
    kf = KFold(n_splits=config.cv_folds, shuffle=True, random_state=config.seed)
    cv_results = []
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(original_data)):
        print(f"\nğŸ“Š ç¬¬ {fold+1}/{config.cv_folds} æŠ˜")
        
        # å‡†å¤‡æ•°æ®
        val_original_indices = set(val_idx)
        train_original = [original_data[i] for i in range(len(original_data))
                         if i not in val_original_indices]
        train_fold = augmented_data + train_original
        val_fold = [original_data[i] for i in val_idx]
        
        print(f"  è®­ç»ƒé›†: {len(train_fold)} (å¢å¼º: {len(augmented_data)}, åŸå§‹: {len(train_original)})")
        print(f"  éªŒè¯é›†: {len(val_fold)}")
        
        # åˆ›å»ºæ¨¡å‹
        model = create_gnn_model(config)
        
        # è®­ç»ƒ
        if ADVANCED_GNN_AVAILABLE and config.use_advanced_gnn:
            # ä½¿ç”¨é«˜çº§GNNçš„è®­ç»ƒæ–¹æ³•
            from torch_geometric.loader import DataLoader
            train_loader = DataLoader(train_fold, batch_size=1, shuffle=True)
            val_loader = DataLoader(val_fold, batch_size=1, shuffle=False)
            
            best_f1 = model.train_model(
                train_loader,
                val_loader,
                epochs=config.gnn_epochs,
                lr=config.gnn_lr,
                device=config.device,
                patience=config.gnn_patience
            )
            
            # è¯„ä¼°
            metrics = model.evaluate(val_loader, config.device)
            best_auc = metrics['auc_pr']
        else:
            # ä½¿ç”¨æ ‡å‡†GNNçš„è®­ç»ƒæ–¹æ³•
            model.to(config.device)
            best_auc, best_f1 = model.train_with_early_stopping(
                train_fold,
                val_fold,
                epochs=config.gnn_epochs,
                lr=config.gnn_lr,
                device=config.device,
                patience=config.gnn_patience
            )
        
        cv_results.append({
            'model': model,
            'val_f1': best_f1,
            'val_auc': best_auc
        })
        
        print(f"  âœ… ç¬¬{fold+1}æŠ˜: F1={best_f1:.4f}, AUC-PR={best_auc:.4f}")
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_result = max(cv_results, key=lambda x: x['val_f1'])
    avg_f1 = np.mean([r['val_f1'] for r in cv_results])
    avg_auc = np.mean([r['val_auc'] for r in cv_results])
    
    print(f"\nâœ… äº¤å‰éªŒè¯å®Œæˆ:")
    print(f"  å¹³å‡F1: {avg_f1:.4f}")
    print(f"  å¹³å‡AUC: {avg_auc:.4f}")
    print(f"  æœ€ä½³F1: {best_result['val_f1']:.4f}")
    
    return best_result['model'], avg_f1, avg_auc, cv_results


def train_and_test_ultimate(train_file, test_files, config):
    """ULTIMATEè®­ç»ƒ-æµ‹è¯•æµç¨‹"""
    train_name = os.path.splitext(os.path.basename(train_file))[0]
    
    print(f"\n{'='*80}")
    print(f"ğŸš€ ULTIMATE PIPELINE: {train_name}")
    print(f"{'='*80}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    ratio_str = f"{config.target_ratio:.2f}".replace(".", "")
    output_dir_name = f"{train_name}_ultimate_r{ratio_str}"
    output_path = os.path.join(config.output_dir, output_dir_name)
    os.makedirs(output_path, exist_ok=True)
    
    total_start_time = time.time()
    
    # ============ é˜¶æ®µ1: åŠ è½½æ•°æ® ============
    print(f"\nğŸ“Š é˜¶æ®µ1: åŠ è½½è®­ç»ƒæ•°æ®")
    train_dataset = load_dataset_quiet(train_file, config)
    
    if not train_dataset:
        print(f"âŒ æ•°æ®é›†ä¸ºç©º")
        return None
    
    print(f"âœ… åŠ è½½äº† {len(train_dataset)} ä¸ªè›‹ç™½è´¨")
    orig_ratio, orig_pos, orig_neg = calculate_class_ratio(train_dataset)
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {orig_pos:,} æ­£ / {orig_neg:,} è´Ÿ (æ¯”ä¾‹: {orig_ratio:.3%})")
    
    # ============ é˜¶æ®µ2: è®­ç»ƒæ‰©æ•£æ¨¡å‹ ============
    diffusion_model = train_enhanced_diffusion_model(train_dataset, config)
    
    # ============ é˜¶æ®µ3: åŠ è½½è¾¹é¢„æµ‹å™¨ ============
    edge_predictor = load_edge_predictor(config)
    if edge_predictor is None:
        print(f"âŒ è¾¹é¢„æµ‹å™¨åŠ è½½å¤±è´¥")
        return None
    
    # ============ é˜¶æ®µ4: æ•°æ®å¢å¼º ============
    print(f"\nğŸ›¡ï¸  é˜¶æ®µ4: ULTIMATE æ•°æ®å¢å¼º")
    augment_start = time.time()
    
    augmented_data, aug_stats = ultimate_augment_dataset(
        train_dataset,
        diffusion_model,
        edge_predictor,
        config
    )
    
    augment_time = time.time() - augment_start
    aug_ratio, aug_pos, aug_neg = calculate_class_ratio(augmented_data)
    
    print(f"\nâœ… å¢å¼ºå®Œæˆ: {aug_pos:,} æ­£ / {aug_neg:,} è´Ÿ (æ¯”ä¾‹: {aug_ratio:.3%})")
    print(f"   ç”¨æ—¶: {augment_time:.1f}ç§’")
    print(f"   è´¨é‡: {aug_stats['avg_quality']:.3f}")
    print(f"   å¤šæ ·æ€§: {aug_stats['avg_diversity']:.3f}")
    
    # ============ é˜¶æ®µ5: è®­ç»ƒGNN ============
    print(f"\nğŸ”„ é˜¶æ®µ5: äº¤å‰éªŒè¯è®­ç»ƒ")
    train_start = time.time()
    
    best_model, avg_f1, avg_auc, cv_results = cross_validation_training(
        augmented_data,
        train_dataset,
        config
    )
    
    train_time = time.time() - train_start
    print(f"   ç”¨æ—¶: {train_time:.1f}ç§’")
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(output_path, "ultimate_gnn_model.pt")
    torch.save(best_model.state_dict(), model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # ============ é˜¶æ®µ6: æµ‹è¯• ============
    print(f"\nğŸ” é˜¶æ®µ6: æ¨¡å‹æµ‹è¯•")
    print(f"{'='*80}")
    
    test_results = {}
    
    for test_file in test_files:
        test_name = os.path.splitext(os.path.basename(test_file))[0]
        print(f"\nğŸ“Š æµ‹è¯•: {test_name}")
        
        try:
            test_dataset = load_dataset_quiet(test_file, config)
            print(f"âœ… åŠ è½½äº† {len(test_dataset)} ä¸ªè›‹ç™½è´¨")
            
            # è¯„ä¼°
            start_time = time.time()
            
            if ADVANCED_GNN_AVAILABLE and config.use_advanced_gnn:
                from torch_geometric.loader import DataLoader
                test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
                metrics = best_model.evaluate(test_loader, config.device)
            else:
                metrics = best_model.evaluate(test_dataset, device=config.device)
            
            eval_time = time.time() - start_time
            
            print(f"ğŸ“ˆ ç»“æœ ({eval_time:.2f}s):")
            print(f"  F1:      {metrics['f1']:.4f}")
            print(f"  MCC:     {metrics['mcc']:.4f}")
            print(f"  ACC:     {metrics['accuracy']:.4f}")
            print(f"  AUC-PR:  {metrics['auc_pr']:.4f}")
            print(f"  AUC-ROC: {metrics['auc_roc']:.4f}")
            
            test_results[test_name] = metrics
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # ============ ä¿å­˜ç»“æœ ============
    total_time = time.time() - total_start_time
    
    full_results = {
        "pipeline": "ULTIMATE",
        "model_name": train_name,
        "training_info": {
            "original_positive": orig_pos,
            "original_negative": orig_neg,
            "original_ratio": orig_ratio,
            "augmented_positive": aug_pos,
            "augmented_negative": aug_neg,
            "augmented_ratio": aug_ratio,
            "target_ratio": config.target_ratio,
            "cv_avg_f1": avg_f1,
            "cv_avg_auc": avg_auc,
            "augmentation_stats": aug_stats,
            "total_time": total_time
        },
        "test_results": test_results
    }
    
    result_file = os.path.join(output_path, "ultimate_results.json")
    with open(result_file, 'w') as f:
        json.dump(full_results, f, indent=2, default=str)
    
    print(f"\nâœ… ULTIMATEè®­ç»ƒ-æµ‹è¯•å®Œæˆ!")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time//60:.0f}m {total_time%60:.0f}s")
    print(f"ğŸ“Š ç»“æœå·²ä¿å­˜: {result_file}")
    
    return full_results


def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ ULTIMATE PIPELINE å¯åŠ¨")
    print(f"{'='*80}")
    
    # é…ç½®
    config = UltimateConfig(target_ratio=0.5, experiment_name="ultimate_v1")
    set_seed(config.seed)
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    train_files = sorted(glob.glob(os.path.join(config.data_dir, "*Train*.txt")))
    test_files = sorted(glob.glob(os.path.join(config.data_dir, "*Test*.txt")))
    test_files = [f for f in test_files if 'DNA-' in os.path.basename(f)]
    
    print(f"\nğŸ” æ‰¾åˆ° {len(train_files)} ä¸ªè®­ç»ƒæ–‡ä»¶")
    print(f"   æ‰¾åˆ° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
    
    # è¿è¡Œpipeline
    all_results = {}
    
    for train_file in train_files:
        try:
            result = train_and_test_ultimate(train_file, test_files, config)
            if result:
                all_results[os.path.basename(train_file)] = result
        except Exception as e:
            print(f"\nâŒ Pipelineå¤±è´¥ ({train_file}): {str(e)}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    if all_results:
        summary_file = os.path.join(config.output_dir, "ultimate_pipeline_summary.json")
        with open(summary_file, 'w') as f:
            json.dump(all_results, f, indent=2, default=str)
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ ULTIMATE PIPELINE å®Œæˆ!")
        print(f"{'='*80}")
        print(f"ğŸ“Š æ±‡æ€»ç»“æœ: {summary_file}")


if __name__ == "__main__":
    main()
