#!/usr/bin/env python3
"""
é«˜çº§GNNæ¨¡å‹ v2.0 - DNAç»“åˆä½ç‚¹é¢„æµ‹ä¸“ç”¨

æ”¹è¿›ç‚¹ï¼š
1. Graph Attention Networks (GAT) - è‡ªé€‚åº”é‚»å±…æƒé‡
2. æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ– - æ›´æ·±ç½‘ç»œï¼Œæ›´å¥½æ¢¯åº¦æµ
3. è¾¹ç‰¹å¾èåˆ - åˆ©ç”¨è¾¹é¢„æµ‹å™¨æä¾›çš„è¾¹æƒé‡
4. å¤šå°ºåº¦ç‰¹å¾èšåˆ - å±€éƒ¨+å…¨å±€ä¿¡æ¯
5. ä¸å¹³è¡¡æ•°æ®ä¸“ç”¨æŸå¤± - Focal Loss + Class Balanced Loss
6. è‡ªé€‚åº”æ­£æ ·æœ¬æƒé‡ - åŠ¨æ€è°ƒæ•´æ ·æœ¬æƒé‡
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATv2Conv, global_mean_pool, global_max_pool
from torch_geometric.utils import degree
from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score, average_precision_score, accuracy_score
import numpy as np


class MultiScaleGATLayer(nn.Module):
    """å¤šå°ºåº¦å›¾æ³¨æ„åŠ›å±‚"""
    def __init__(self, in_dim, out_dim, heads=4, dropout=0.3):
        super().__init__()

        # å¤šå¤´æ³¨æ„åŠ›
        self.gat = GATv2Conv(
            in_dim,
            out_dim // heads,
            heads=heads,
            dropout=dropout,
            edge_dim=1,  # æ”¯æŒè¾¹ç‰¹å¾
            concat=True
        )

        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(out_dim)

        # æ®‹å·®æŠ•å½±ï¼ˆå¦‚æœç»´åº¦ä¸åŒ¹é…ï¼‰
        self.residual_proj = nn.Linear(in_dim, out_dim) if in_dim != out_dim else None

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index, edge_attr=None):
        """
        Args:
            x: (num_nodes, in_dim)
            edge_index: (2, num_edges)
            edge_attr: (num_edges, 1) è¾¹æƒé‡/ç‰¹å¾
        """
        # æ®‹å·®è¿æ¥
        identity = x
        if self.residual_proj is not None:
            identity = self.residual_proj(identity)

        # GAT
        x = self.gat(x, edge_index, edge_attr=edge_attr)

        # æ®‹å·® + LayerNorm + Dropout
        x = self.layer_norm(x + identity)
        x = self.dropout(x)

        return x


class AdvancedBindingSiteGNN(nn.Module):
    """é«˜çº§DNAç»“åˆä½ç‚¹é¢„æµ‹GNN"""
    def __init__(
        self,
        input_dim=1280,
        hidden_dim=256,
        num_layers=4,
        heads=4,
        dropout=0.3,
        use_edge_features=True,
        focal_alpha=0.25,
        focal_gamma=2.0,
        class_balanced=True
    ):
        super().__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.use_edge_features = use_edge_features
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        self.class_balanced = class_balanced

        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )

        # å¤šå±‚GAT
        self.gat_layers = nn.ModuleList([
            MultiScaleGATLayer(
                hidden_dim,
                hidden_dim,
                heads=heads,
                dropout=dropout
            )
            for _ in range(num_layers)
        ])

        # å…¨å±€ä¿¡æ¯èšåˆ
        self.global_pool = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),  # *2 for mean+max pooling
            nn.LayerNorm(hidden_dim),
            nn.GELU()
        )

        # å±€éƒ¨-å…¨å±€èåˆ
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),  # local + global
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )

        # è¾“å‡ºå¤´
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )

        # è¾¹ç‰¹å¾ç¼–ç å™¨ï¼ˆå¦‚æœä½¿ç”¨è¾¹ç‰¹å¾ï¼‰
        if use_edge_features:
            self.edge_encoder = nn.Sequential(
                nn.Linear(1, 16),
                nn.ReLU(),
                nn.Linear(16, 1)
            )

    def forward(self, data):
        """
        Args:
            data: PyG Data object
        Returns:
            logits: (num_nodes,)
        """
        x, edge_index = data.x, data.edge_index
        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)

        # è¾¹ç‰¹å¾
        if self.use_edge_features and hasattr(data, 'edge_attr') and data.edge_attr is not None:
            edge_attr = self.edge_encoder(data.edge_attr)
        else:
            edge_attr = None

        # è¾“å…¥æŠ•å½±
        x = self.input_proj(x)  # (num_nodes, hidden_dim)

        # å¤šå±‚GATï¼ˆå±€éƒ¨ä¿¡æ¯ï¼‰
        for gat_layer in self.gat_layers:
            x = gat_layer(x, edge_index, edge_attr)

        # å…¨å±€ä¿¡æ¯
        global_mean = global_mean_pool(x, batch)  # (num_graphs, hidden_dim)
        global_max = global_max_pool(x, batch)    # (num_graphs, hidden_dim)
        global_info = torch.cat([global_mean, global_max], dim=1)  # (num_graphs, hidden_dim*2)
        global_info = self.global_pool(global_info)  # (num_graphs, hidden_dim)

        # å¹¿æ’­å…¨å±€ä¿¡æ¯åˆ°æ‰€æœ‰èŠ‚ç‚¹
        global_info_expanded = global_info[batch]  # (num_nodes, hidden_dim)

        # å±€éƒ¨-å…¨å±€èåˆ
        x_fused = torch.cat([x, global_info_expanded], dim=1)  # (num_nodes, hidden_dim*2)
        x = self.fusion(x_fused)  # (num_nodes, hidden_dim)

        # åˆ†ç±»
        logits = self.classifier(x).squeeze(-1)  # (num_nodes,)

        return logits

    def compute_loss(self, logits, targets, effective_num=None):
        """
        è®¡ç®—æŸå¤±ï¼ˆFocal Loss + Class Balancedï¼‰

        Args:
            logits: (num_nodes,)
            targets: (num_nodes,) 0/1
            effective_num: dict with 'num_pos', 'num_neg' for class balancing
        """
        # Focal Loss
        bce_loss = F.binary_cross_entropy_with_logits(
            logits,
            targets.float(),
            reduction='none'
        )

        probs = torch.sigmoid(logits)
        pt = targets * probs + (1 - targets) * (1 - probs)
        focal_weight = (1 - pt) ** self.focal_gamma

        alpha_t = targets * self.focal_alpha + (1 - targets) * (1 - self.focal_alpha)
        focal_loss = alpha_t * focal_weight * bce_loss

        # Class Balanced Loss (å¯é€‰)
        if self.class_balanced and effective_num is not None:
            num_pos = effective_num.get('num_pos', 1)
            num_neg = effective_num.get('num_neg', 1)
            beta = 0.9999  # è¶…å‚æ•°

            # Effective number
            effective_num_pos = (1 - beta ** num_pos) / (1 - beta)
            effective_num_neg = (1 - beta ** num_neg) / (1 - beta)

            # Class weights
            weight_pos = effective_num_neg / (effective_num_pos + effective_num_neg)
            weight_neg = effective_num_pos / (effective_num_pos + effective_num_neg)

            # åº”ç”¨æƒé‡
            class_weights = targets * weight_pos + (1 - targets) * weight_neg
            focal_loss = focal_loss * class_weights

        return focal_loss.mean()

    def train_model(self, train_loader, val_loader, epochs=100, lr=1e-3,
                   device='cuda', patience=15, save_path=None):
        """è®­ç»ƒæ¨¡å‹"""
        self.to(device)
        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='max',
            factor=0.5,
            patience=5
        )

        best_f1 = 0
        patience_counter = 0

        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ Advanced GNN (layers={self.num_layers}, hidden={self.hidden_dim})")

        for epoch in range(epochs):
            # è®­ç»ƒ
            self.train()
            total_loss = 0
            num_batches = 0

            # ç»Ÿè®¡ç±»åˆ«æ•°é‡ï¼ˆç”¨äºclass balanced lossï¼‰
            total_pos = 0
            total_neg = 0

            for batch_data in train_loader:
                batch_data = batch_data.to(device)

                # ç»Ÿè®¡
                total_pos += (batch_data.y == 1).sum().item()
                total_neg += (batch_data.y == 0).sum().item()

                # å‰å‘
                logits = self(batch_data)
                loss = self.compute_loss(
                    logits,
                    batch_data.y,
                    effective_num={'num_pos': total_pos, 'num_neg': total_neg}
                )

                # åå‘
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)
                optimizer.step()

                total_loss += loss.item()
                num_batches += 1

            avg_loss = total_loss / max(num_batches, 1)

            # éªŒè¯
            if epoch % 5 == 0 or epoch == epochs - 1:
                val_metrics = self.evaluate(val_loader, device)
                scheduler.step(val_metrics['f1'])

                print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - "
                      f"Val F1: {val_metrics['f1']:.4f} - "
                      f"Val MCC: {val_metrics['mcc']:.4f} - "
                      f"Val AUC-PR: {val_metrics['auc_pr']:.4f}")

                # æ—©åœ
                if val_metrics['f1'] > best_f1:
                    best_f1 = val_metrics['f1']
                    patience_counter = 0

                    if save_path:
                        torch.save(self.state_dict(), save_path)
                        print(f"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ (F1={best_f1:.4f})")
                else:
                    patience_counter += 1

                if patience_counter >= patience:
                    print(f"  â¹ï¸  æ—©åœ (patience={patience})")
                    break

        print(f"âœ… è®­ç»ƒå®Œæˆ - æœ€ä½³F1: {best_f1:.4f}")
        return best_f1

    def evaluate(self, data_loader, device='cuda'):
        """è¯„ä¼°æ¨¡å‹"""
        self.eval()
        all_preds = []
        all_probs = []
        all_targets = []

        with torch.no_grad():
            for batch_data in data_loader:
                batch_data = batch_data.to(device)
                logits = self(batch_data)
                probs = torch.sigmoid(logits)
                preds = (probs > 0.5).long()

                all_preds.append(preds.cpu())
                all_probs.append(probs.cpu())
                all_targets.append(batch_data.y.cpu())

        all_preds = torch.cat(all_preds).numpy()
        all_probs = torch.cat(all_probs).numpy()
        all_targets = torch.cat(all_targets).numpy()

        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'f1': f1_score(all_targets, all_preds, zero_division=0),
            'mcc': matthews_corrcoef(all_targets, all_preds),
            'accuracy': accuracy_score(all_targets, all_preds),
            'auc_pr': average_precision_score(all_targets, all_probs) if len(np.unique(all_targets)) > 1 else 0,
            'auc_roc': roc_auc_score(all_targets, all_probs) if len(np.unique(all_targets)) > 1 else 0
        }

        return metrics
