#!/usr/bin/env python3
"""
æ”¹è¿›çš„GNNæ¨¡åž‹ - é’ˆå¯¹æµ‹è¯•æ€§èƒ½ä¼˜åŒ–
åŒ…å«Focal Lossã€æ›´å¼ºæ­£åˆ™åŒ–ã€æ¦‚çŽ‡æ ¡å‡†ç­‰æŠ€æœ¯
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, GCNConv, SAGEConv
from sklearn.metrics import (f1_score, matthews_corrcoef, precision_recall_curve, auc,
                             accuracy_score, precision_score, recall_score, 
                             roc_auc_score, confusion_matrix, balanced_accuracy_score)
import numpy as np
import os


class ImprovedResidualBlock(nn.Module):
    """æ”¹è¿›çš„æ®‹å·®å— - å¢žå¼ºæ­£åˆ™åŒ–"""
    def __init__(self, in_dim, out_dim, dropout=0.5):
        super().__init__()
        self.linear = nn.Sequential(
            nn.Linear(in_dim, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(out_dim, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.Dropout(dropout * 0.5)  # ç¬¬äºŒå±‚dropoutç¨å¾®ä½Žä¸€ç‚¹
        )
        self.shortcut = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return F.relu(self.linear(x) + self.shortcut(x))


class ImprovedBindingSiteGNN(nn.Module):
    """æ”¹è¿›çš„ç»‘å®šä½ç‚¹GNN - é’ˆå¯¹æµ‹è¯•æ€§èƒ½ä¼˜åŒ–"""
    
    def __init__(self, input_dim=1280, hidden_dim=256, dropout=0.5, 
                 use_focal_loss=True, focal_alpha=0.25, focal_gamma=2.0, pos_weight=1.5):
        super().__init__()
        
        self.use_focal_loss = use_focal_loss
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        
        # è¾“å…¥æŠ•å½±å±‚ - å¢žåŠ æ­£åˆ™åŒ–
        self.input_proj = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout * 0.7)  # è¾“å…¥å±‚ç¨å¾®ä½Žä¸€ç‚¹çš„dropout
        )

        # å›¾å·ç§¯å±‚ - ç¡®ä¿æ‰€æœ‰è¾“å‡ºç»´åº¦ä¸€è‡´
        self.conv1 = GATConv(hidden_dim, hidden_dim, heads=1, dropout=dropout, concat=False)  # å•å¤´GATï¼Œè¾“å‡ºç»´åº¦=hidden_dim
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = SAGEConv(hidden_dim, hidden_dim)

        # æ®‹å·®å— - å‡å°‘å±‚æ•°
        self.res_blocks = nn.ModuleList([
            ImprovedResidualBlock(hidden_dim, hidden_dim, dropout) for _ in range(1)  # åªç”¨1ä¸ªæ®‹å·®å—
        ])

        # è¾“å‡ºå±‚ - ç®€åŒ–ç»“æž„
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 128),  # å‡å°‘éšè—å±‚å¤§å°
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(128),
            nn.Linear(128, 64),  # å¢žåŠ ä¸€å±‚ä½†å‡å°‘å‚æ•°
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(64, 1)
        )

        # æŸå¤±å‡½æ•°
        if use_focal_loss:
            # ä¸ä½¿ç”¨pos_weightï¼Œè®©Focal Lossè‡ªåŠ¨å¤„ç†
            self.loss_fn = self.focal_loss
        else:
            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))
        
        # æ¸©åº¦å‚æ•°ç”¨äºŽæ¦‚çŽ‡æ ¡å‡†
        self.temperature = nn.Parameter(torch.ones(1))

    def focal_loss(self, pred, target):
        """Focal Losså®žçŽ°"""
        ce_loss = F.binary_cross_entropy_with_logits(pred, target.float(), reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.focal_alpha * (1 - pt) ** self.focal_gamma * ce_loss
        return focal_loss.mean()

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        # è¾“å…¥æŠ•å½±
        x = self.input_proj(x)

        # ðŸ”§ ä¿®å¤ï¼šéªŒè¯å’Œæ¸…ç†è¾¹ç´¢å¼•
        num_nodes = x.size(0)

        if edge_index.numel() == 0:
            # ç©ºè¾¹åˆ—è¡¨ - åˆ›å»ºè‡ªå¾ªçŽ¯
            edge_index = torch.arange(num_nodes, device=x.device, dtype=torch.long)
            edge_index = torch.stack([edge_index, edge_index], dim=0)
        else:
            # æ£€æŸ¥è¾¹çš„æœ‰æ•ˆæ€§
            max_idx = edge_index.max().item()
            if max_idx >= num_nodes:
                # ç§»é™¤è¶…å‡ºèŒƒå›´çš„è¾¹
                valid_mask = (edge_index[0] < num_nodes) & (edge_index[1] < num_nodes)
                edge_index = edge_index[:, valid_mask]

                # å¦‚æžœæ²¡æœ‰æœ‰æ•ˆè¾¹ï¼Œåˆ›å»ºè‡ªå¾ªçŽ¯
                if edge_index.numel() == 0:
                    edge_index = torch.arange(num_nodes, device=x.device, dtype=torch.long)
                    edge_index = torch.stack([edge_index, edge_index], dim=0)

        # å¤šç±»åž‹å›¾å·ç§¯ - ä½¿ç”¨residual connection
        identity = x
        x1 = F.elu(self.conv1(x, edge_index))
        x2 = F.elu(self.conv2(x, edge_index))
        x3 = F.elu(self.conv3(x, edge_index))
        x = x1 + x2 + x3 + identity  # æ·»åŠ è¾“å…¥residual

        # æ®‹å·®å—
        for block in self.res_blocks:
            x = block(x)

        # åˆ†ç±»è¾“å‡º
        logits = self.classifier(x).squeeze()
        
        # è®­ç»ƒæ—¶è¿”å›žåŽŸå§‹logitsï¼ŒæŽ¨ç†æ—¶ä½¿ç”¨æ¸©åº¦æ ¡å‡†
        if self.training:
            return logits
        else:
            return logits / self.temperature  # æ¸©åº¦æ ¡å‡†

    def train_model(self, train_data, val_data, epochs=30, lr=5e-4, device='cpu', patience=5):
        """è®­ç»ƒæ¨¡åž‹ - æ”¹è¿›ç‰ˆ"""
        self.to(device)
        
        # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œå¢žåŠ æƒé‡è¡°å‡
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=lr, 
            weight_decay=1e-3,  # å¢žåŠ æƒé‡è¡°å‡
            betas=(0.9, 0.999)
        )
        
        # ä½¿ç”¨æ›´æ¿€è¿›çš„å­¦ä¹ çŽ‡è°ƒåº¦
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.7, patience=3
        )

        best_val_auc = 0
        best_val_f1 = 0
        no_improve = 0

        for epoch in range(epochs):
            self.train()
            total_loss = 0
            batch_count = 0

            for data in train_data:
                if data.x.size(0) == 0:
                    continue

                data = data.to(device)
                optimizer.zero_grad()
                out = self(data)

                if (data.y == 1).sum().item() == 0:
                    continue

                loss = self.loss_fn(out, data.y.float())
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.parameters(), 0.5)  # æ›´æ¿€è¿›çš„æ¢¯åº¦è£å‰ª

                optimizer.step()
                total_loss += loss.item()
                batch_count += 1

            avg_loss = total_loss / batch_count if batch_count > 0 else 0

            # éªŒè¯
            val_metrics = self.evaluate(val_data, device)
            val_f1 = val_metrics['f1']
            val_auc_pr = val_metrics['auc_pr']

            print(f"Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.4f} | "
                  f"Val F1: {val_f1:.4f} | Val AUC-PR: {val_auc_pr:.4f} | "
                  f"Val ACC: {val_metrics['accuracy']:.4f} | "
                  f"LR: {optimizer.param_groups[0]['lr']:.2e} | "
                  f"Temp: {self.temperature.item():.3f}")

            # å­¦ä¹ çŽ‡è°ƒåº¦
            scheduler.step(val_auc_pr)

            # ä¿å­˜æœ€ä½³æ¨¡åž‹
            if val_auc_pr > best_val_auc:
                best_val_auc = val_auc_pr
                best_val_f1 = val_f1
                torch.save(self.state_dict(), "best_improved_gnn_model.pt")
                no_improve = 0
            else:
                no_improve += 1
                if no_improve >= patience:
                    print(f"Early stopping at epoch {epoch + 1}")
                    break

        # åŠ è½½æœ€ä½³æ¨¡åž‹
        if os.path.exists("best_improved_gnn_model.pt"):
            self.load_state_dict(torch.load("best_improved_gnn_model.pt"))
            
        print(f"Training complete. Best Val AUC-PR: {best_val_auc:.4f}, Best Val F1: {best_val_f1:.4f}")
        return best_val_auc, best_val_f1

    def evaluate(self, dataset, device='cpu'):
        """è¯„ä¼°å‡½æ•° - åŒ…å«æ¦‚çŽ‡æ ¡å‡†"""
        if not dataset:
            return {'f1': 0, 'mcc': 0, 'auc_pr': 0, 'accuracy': 0, 'balanced_accuracy': 0,
                   'precision': 0, 'recall': 0, 'specificity': 0, 'auc_roc': 0}

        self.eval()
        self.to(device)
        all_probs = []
        all_labels = []

        with torch.no_grad():
            for data in dataset:
                if data.x.size(0) == 0:
                    continue

                data = data.to(device)
                out = self(data)  # è‡ªåŠ¨åº”ç”¨æ¸©åº¦æ ¡å‡†
                probs = torch.sigmoid(out)

                all_probs.extend(probs.cpu().tolist())
                all_labels.extend(data.y.cpu().tolist())

        if len(all_labels) == 0:
            return {'f1': 0, 'mcc': 0, 'auc_pr': 0, 'accuracy': 0, 'balanced_accuracy': 0,
                   'precision': 0, 'recall': 0, 'specificity': 0, 'auc_roc': 0}

        all_labels = [int(label) for label in all_labels]
        
        # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼ - é’ˆå¯¹ä¸åŒæŒ‡æ ‡
        thresholds = np.arange(0.05, 0.96, 0.05)
        best_metrics = {
            'f1': {'value': 0, 'threshold': 0.5},
            'balanced_acc': {'value': 0, 'threshold': 0.5}
        }
        
        for threshold in thresholds:
            preds = [1 if p > threshold else 0 for p in all_probs]
            
            f1 = f1_score(all_labels, preds, zero_division=0)
            balanced_acc = balanced_accuracy_score(all_labels, preds)
            
            if f1 > best_metrics['f1']['value']:
                best_metrics['f1']['value'] = f1
                best_metrics['f1']['threshold'] = threshold
                
            if balanced_acc > best_metrics['balanced_acc']['value']:
                best_metrics['balanced_acc']['value'] = balanced_acc
                best_metrics['balanced_acc']['threshold'] = threshold

        # ä½¿ç”¨å¹³è¡¡å‡†ç¡®çŽ‡æœ€ä¼˜é˜ˆå€¼è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        best_threshold = best_metrics['balanced_acc']['threshold']
        final_preds = [1 if p > best_threshold else 0 for p in all_probs]
        
        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, final_preds)
        balanced_acc = balanced_accuracy_score(all_labels, final_preds)
        precision = precision_score(all_labels, final_preds, zero_division=0)
        recall = recall_score(all_labels, final_preds, zero_division=0)
        f1 = f1_score(all_labels, final_preds, zero_division=0)
        mcc = matthews_corrcoef(all_labels, final_preds)
        
        tn, fp, fn, tp = confusion_matrix(all_labels, final_preds, labels=[0, 1]).ravel()
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0

        # AUCæŒ‡æ ‡
        auc_pr = float('nan')
        auc_roc = float('nan')
        
        if any(label == 1 for label in all_labels):
            try:
                precision_curve, recall_curve, _ = precision_recall_curve(all_labels, all_probs)
                auc_pr = auc(recall_curve, precision_curve)
                auc_roc = roc_auc_score(all_labels, all_probs)
            except:
                pass

        return {
            'f1': f1,
            'accuracy': accuracy,
            'balanced_accuracy': balanced_acc,
            'precision': precision,
            'recall': recall,
            'specificity': specificity,
            'sensitivity': recall,
            'mcc': mcc,
            'auc_pr': auc_pr,
            'auc_roc': auc_roc,
            'confusion_matrix': {'tp': int(tp), 'tn': int(tn), 'fp': int(fp), 'fn': int(fn)},
            'best_threshold_f1': best_metrics['f1']['threshold'],
            'best_threshold_balanced_acc': best_metrics['balanced_acc']['threshold'],
            'total_samples': len(all_labels),
            'positive_samples': sum(all_labels),
            'negative_samples': len(all_labels) - sum(all_labels),
            'positive_ratio': sum(all_labels) / len(all_labels) if len(all_labels) > 0 else 0
        }