#!/usr/bin/env python3
"""
ä½¿ç”¨ESM2æå–è›‹ç™½è´¨ç‰¹å¾
ESM2 (Evolutionary Scale Modeling 2) æ˜¯Meta AIå¼€å‘çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹
"""
import os
import sys
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel

current_dir = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(current_dir, "data")
PPI_RAW_DIR = os.path.join(DATA_DIR, "ppi_raw")
PPI_PROCESSED_DIR = os.path.join(DATA_DIR, "ppi_processed")
os.makedirs(PPI_PROCESSED_DIR, exist_ok=True)


def load_sequences(fasta_file):
    """ä»FASTAæ–‡ä»¶åŠ è½½åºåˆ—"""
    print(f"ğŸ“– åŠ è½½FASTAæ–‡ä»¶: {fasta_file}")

    sequences = {}
    current_id = None
    current_seq = []

    with open(fasta_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith('>'):
                # ä¿å­˜å‰ä¸€ä¸ªåºåˆ—
                if current_id is not None:
                    sequences[current_id] = ''.join(current_seq)

                # æå–è›‹ç™½è´¨ID
                current_id = line[1:].split()[0]
                current_seq = []
            else:
                current_seq.append(line)

        # ä¿å­˜æœ€åä¸€ä¸ªåºåˆ—
        if current_id is not None:
            sequences[current_id] = ''.join(current_seq)

    print(f"âœ… åŠ è½½å®Œæˆ: {len(sequences):,} æ¡åºåˆ—")
    return sequences


def extract_esm2_features(sequences, model_name="facebook/esm2_t33_650M_UR50D", batch_size=8):
    """
    ä½¿ç”¨ESM2æå–è›‹ç™½è´¨ç‰¹å¾

    å‚æ•°:
        sequences: dict, {protein_id: sequence}
        model_name: ESM2æ¨¡å‹åç§°
        batch_size: æ‰¹å¤„ç†å¤§å° (æ ¹æ®GPUå†…å­˜è°ƒæ•´)
    """
    print(f"\nğŸ§¬ ä½¿ç”¨ESM2æå–ç‰¹å¾...")
    print(f"   æ¨¡å‹: {model_name}")
    print(f"   æ‰¹å¤§å°: {batch_size}")

    # æ£€æµ‹è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   è®¾å¤‡: {device}")

    if device.type == "cpu":
        print("   âš ï¸  è­¦å‘Š: ä½¿ç”¨CPUï¼Œé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼å»ºè®®ä½¿ç”¨GPU")

    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print("\nğŸ“¥ åŠ è½½ESM2æ¨¡å‹ (é¦–æ¬¡åŠ è½½ä¼šä¸‹è½½ ~2.5GB)...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        model = model.to(device)
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. å®‰è£…transformers: pip install transformers")
        print("   3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åé‡è¯•")
        return None

    # å‡†å¤‡åºåˆ—åˆ—è¡¨ (æŒ‰è›‹ç™½è´¨IDæ’åºä»¥ä¿æŒä¸€è‡´æ€§)
    protein_ids = sorted(sequences.keys())
    sequences_list = [sequences[pid] for pid in protein_ids]

    print(f"\nğŸ”„ æå– {len(sequences_list):,} ä¸ªè›‹ç™½è´¨çš„ç‰¹å¾...")
    print(f"   é¢„è®¡æ—¶é—´: ~{len(sequences_list) * 0.5 / batch_size / 60:.1f} åˆ†é’Ÿ (GPU)")

    all_features = []

    with torch.no_grad():
        for i in tqdm(range(0, len(sequences_list), batch_size), desc="æå–ç‰¹å¾"):
            batch_seqs = sequences_list[i:i+batch_size]

            try:
                # åˆ†è¯ (æˆªæ–­åˆ°1024ä¸ªtoken)
                inputs = tokenizer(
                    batch_seqs,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=1024
                )
                inputs = {k: v.to(device) for k, v in inputs.items()}

                # è·å–æ¨¡å‹è¾“å‡º
                outputs = model(**inputs, output_hidden_states=True)

                # ä½¿ç”¨æœ€åä¸€å±‚çš„å¹³å‡æ± åŒ–ä½œä¸ºç‰¹å¾
                last_hidden = outputs.last_hidden_state  # [batch_size, seq_len, 1280]

                # å»æ‰ç‰¹æ®Štoken (padding) åè¿›è¡Œå¹³å‡æ± åŒ–
                mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden.shape).float()
                masked_hidden = last_hidden * mask
                sum_hidden = masked_hidden.sum(dim=1)
                lengths = mask.sum(dim=1)
                batch_features = (sum_hidden / lengths).cpu().numpy()

                all_features.append(batch_features)

            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"\nâš ï¸  GPUå†…å­˜ä¸è¶³ï¼Œå‡å°batch_sizeå¹¶é‡è¯•")
                    print(f"   å½“å‰batch_size={batch_size}, å»ºè®®å‡å°åˆ° {batch_size // 2}")
                    torch.cuda.empty_cache()
                    return None
                else:
                    raise e

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    features = np.vstack(all_features).astype(np.float32)

    print(f"\nâœ… ç‰¹å¾æå–å®Œæˆ!")
    print(f"   â€¢ å½¢çŠ¶: {features.shape}")
    print(f"   â€¢ ç±»å‹: {features.dtype}")
    print(f"   â€¢ å¤§å°: {features.nbytes / 1024 / 1024:.1f} MB")
    print(f"\nğŸ“Š ç‰¹å¾ç»Ÿè®¡:")
    print(f"   â€¢ å‡å€¼: {features.mean():.6f}")
    print(f"   â€¢ æ ‡å‡†å·®: {features.std():.6f}")
    print(f"   â€¢ æœ€å°å€¼: {features.min():.6f}")
    print(f"   â€¢ æœ€å¤§å€¼: {features.max():.6f}")

    return features, protein_ids


def save_features(features, protein_ids, output_dir):
    """ä¿å­˜ç‰¹å¾"""
    print(f"\nğŸ’¾ ä¿å­˜ç‰¹å¾...")

    # ä¿å­˜ç‰¹å¾çŸ©é˜µ
    features_file = os.path.join(output_dir, "features.npy")
    np.save(features_file, features)
    print(f"âœ… ç‰¹å¾å·²ä¿å­˜: {features_file}")

    # ä¿å­˜è›‹ç™½è´¨IDæ˜ å°„
    protein_to_idx = {pid: i for i, pid in enumerate(protein_ids)}
    idx_to_protein = {i: pid for pid, i in protein_to_idx.items()}

    import json
    mapping_file = os.path.join(output_dir, "protein_mapping.json")
    with open(mapping_file, 'w') as f:
        json.dump({
            'protein_to_idx': protein_to_idx,
            'idx_to_protein': idx_to_protein,
            'num_proteins': len(protein_ids),
            'feature_dim': features.shape[1]
        }, f, indent=2)

    print(f"âœ… æ˜ å°„å·²ä¿å­˜: {mapping_file}")


def main():
    print("ğŸ§¬ æ­¥éª¤3: ä½¿ç”¨ESM2æå–è›‹ç™½è´¨ç‰¹å¾")
    print("=" * 70)
    print()

    # åŠ è½½åºåˆ—
    fasta_file = os.path.join(PPI_RAW_DIR, "protein_sequences.fasta")
    if not os.path.exists(fasta_file):
        print(f"âŒ é”™è¯¯: åºåˆ—æ–‡ä»¶ä¸å­˜åœ¨: {fasta_file}")
        print("è¯·å…ˆè¿è¡Œ: python 2_download_protein_sequences.py")
        return False

    sequences = load_sequences(fasta_file)

    # æå–ESM2ç‰¹å¾
    # æ³¨æ„: batch_sizeæ ¹æ®GPUå†…å­˜è°ƒæ•´
    # - 16GB GPU: batch_size=16
    # - 24GB GPU: batch_size=24
    # - 40GB GPU: batch_size=32
    # - CPU: batch_size=1 (éå¸¸æ…¢)

    batch_size = 64  # é€‚åˆA100 40GB
    result = extract_esm2_features(sequences, batch_size=batch_size)

    if result is None:
        print("\nâŒ ç‰¹å¾æå–å¤±è´¥")
        return False

    features, protein_ids = result

    # ä¿å­˜ç‰¹å¾
    save_features(features, protein_ids, PPI_PROCESSED_DIR)

    print("\n" + "=" * 70)
    print("âœ… æ­¥éª¤3å®Œæˆ: ESM2ç‰¹å¾å·²æå–")
    print(f"ğŸ“ ç‰¹å¾ä½ç½®: {PPI_PROCESSED_DIR}")
    print(f"   â€¢ ç‰¹å¾æ–‡ä»¶: features.npy ({features.shape})")
    print(f"   â€¢ æ˜ å°„æ–‡ä»¶: protein_mapping.json")
    print("\nğŸ‘‰ ä¸‹ä¸€æ­¥: é¢„å¤„ç†PPIæ•°æ®")
    print("   è¿è¡Œ: python 4_preprocess_ppi_data.py")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
