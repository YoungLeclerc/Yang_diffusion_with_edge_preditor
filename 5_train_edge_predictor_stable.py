#!/usr/bin/env python3
"""
æ­¥éª¤5: è®­ç»ƒè¾¹é¢„æµ‹å™¨ï¼ˆç¨³å®šç‰ˆ - ä½¿ç”¨Warmup + æ›´ä½å­¦ä¹ ç‡ï¼‰
"""
import os
import sys
import json
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm

# å¯¼å…¥è¾¹é¢„æµ‹å™¨æ¨¡å‹
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, parent_dir)
from edge_predictor_augmentation import ImprovedEdgePredictor

# å¯¼å…¥ç¨³å®šé…ç½®
current_dir = os.path.dirname(os.path.abspath(__file__))
import importlib.util
config_path = os.path.join(current_dir, "ppi_config_stable.py")
spec = importlib.util.spec_from_file_location("ppi_config_stable", config_path)
config = importlib.util.module_from_spec(spec)
spec.loader.exec_module(config)

FEATURE_DIM = config.FEATURE_DIM
DEVICE = config.DEVICE
EPOCHS = config.TRAIN_EPOCHS
BATCH_SIZE = config.BATCH_SIZE
PPI_PROCESSED_DIR = config.PPI_PROCESSED_DIR


class WarmupCosineScheduler:
    """Warmup + Cosineé€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""

    def __init__(self, optimizer, warmup_epochs, total_epochs, base_lr, min_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.current_epoch = 0

    def step(self):
        """æ›´æ–°å­¦ä¹ ç‡"""
        self.current_epoch += 1

        if self.current_epoch <= self.warmup_epochs:
            # Warmupé˜¶æ®µï¼šçº¿æ€§å¢é•¿
            lr = self.base_lr * self.current_epoch / self.warmup_epochs
        else:
            # Cosineé€€ç«é˜¶æ®µ
            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))

        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

        return lr

    def get_last_lr(self):
        """è¿”å›å½“å‰å­¦ä¹ ç‡"""
        return [group['lr'] for group in self.optimizer.param_groups]


class StableEdgePredictorTrainer:
    """ç¨³å®šç‰ˆè¾¹é¢„æµ‹å™¨è®­ç»ƒå™¨"""

    def __init__(self):
        self.device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆå§‹åŒ–æ¨¡å‹
        hidden_dim = getattr(config, 'HIDDEN_DIM', 1024)
        self.model = ImprovedEdgePredictor(input_dim=FEATURE_DIM, hidden_dim=hidden_dim).to(self.device)

        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {trainable_params:,} / {total_params:,} (å¯è®­ç»ƒ/æ€»è®¡)")

        # ä¼˜åŒ–å™¨ï¼ˆç¨³å®šç‰ˆå‚æ•°ï¼‰
        lr = getattr(config, 'LEARNING_RATE', 0.0005)
        weight_decay = getattr(config, 'WEIGHT_DECAY', 1e-4)
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999)  # æ›´ç¨³å®šçš„betaå‚æ•°
        )

        print(f"ğŸ“ˆ å­¦ä¹ ç‡é…ç½®:")
        print(f"   â€¢ åŸºç¡€å­¦ä¹ ç‡: {lr}")
        print(f"   â€¢ Weight Decay: {weight_decay}")

        # Warmup + Cosineé€€ç«è°ƒåº¦å™¨
        warmup_epochs = getattr(config, 'LR_WARMUP_EPOCHS', 3)
        min_lr = getattr(config, 'LR_MIN', 1e-6)
        self.scheduler = WarmupCosineScheduler(
            self.optimizer,
            warmup_epochs=warmup_epochs,
            total_epochs=EPOCHS,
            base_lr=lr,
            min_lr=min_lr
        )
        print(f"   â€¢ Warmup: {warmup_epochs} epochs")
        print(f"   â€¢ æœ€å°å­¦ä¹ ç‡: {min_lr}")

        # æŸå¤±å‡½æ•°
        self.criterion = nn.BCELoss()

        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = getattr(config, 'USE_AMP', True)
        self.scaler = torch.cuda.amp.GradScaler() if self.use_amp else None
        if self.use_amp:
            print(f"âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")

        # æ¢¯åº¦è£å‰ª
        self.gradient_clip_norm = getattr(config, 'GRADIENT_CLIP_NORM', 0.5)
        print(f"âœ‚ï¸  æ¢¯åº¦è£å‰ª: {self.gradient_clip_norm}")

        # æ¢¯åº¦ç´¯ç§¯
        self.gradient_accumulation = getattr(config, 'GRADIENT_ACCUMULATION', 1)

        # Early Stopping
        self.patience = getattr(config, 'EARLY_STOPPING_PATIENCE', 10)
        self.min_delta = getattr(config, 'MIN_DELTA', 0.0001)
        print(f"â¸ï¸  Early Stopping: patience={self.patience}, min_delta={self.min_delta}")

        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'val_auc': [],
            'val_acc': [],
            'val_precision': [],
            'val_recall': [],
            'val_f1': [],
            'learning_rate': []
        }

        self.best_auc = 0.0
        self.best_epoch = 0
        self.no_improve_count = 0

    def load_data(self):
        """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
        print("\nğŸ“Š åŠ è½½æ•°æ®...")

        # åŠ è½½è¾¹
        edges_train = np.load(os.path.join(PPI_PROCESSED_DIR, "edges_train.npy"))
        edges_val = np.load(os.path.join(PPI_PROCESSED_DIR, "edges_val.npy"))

        # åŠ è½½æ ‡ç­¾
        labels_train = np.load(os.path.join(PPI_PROCESSED_DIR, "labels_train.npy"))
        labels_val = np.load(os.path.join(PPI_PROCESSED_DIR, "labels_val.npy"))

        # åŠ è½½ç‰¹å¾
        features = np.load(os.path.join(PPI_PROCESSED_DIR, "features.npy"))

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   â€¢ è®­ç»ƒè¾¹: {len(edges_train):,}")
        print(f"     â””â”€ æ­£: {(labels_train==1).sum():,}, è´Ÿ: {(labels_train==0).sum():,}")
        print(f"   â€¢ éªŒè¯è¾¹: {len(edges_val):,}")
        print(f"     â””â”€ æ­£: {(labels_val==1).sum():,}, è´Ÿ: {(labels_val==0).sum():,}")
        print(f"   â€¢ ç‰¹å¾: {features.shape}")

        return edges_train, labels_train, edges_val, labels_val, features

    def create_dataloader(self, edges, labels, features, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        src_feats = torch.tensor(features[edges[:, 0]], dtype=torch.float32)
        dst_feats = torch.tensor(features[edges[:, 1]], dtype=torch.float32)
        labels_tensor = torch.tensor(labels, dtype=torch.float32)

        dataset = TensorDataset(src_feats, dst_feats, labels_tensor)

        num_workers = getattr(config, 'NUM_WORKERS', 16)

        dataloader = DataLoader(
            dataset,
            batch_size=BATCH_SIZE,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=True,
            persistent_workers=True if num_workers > 0 else False,
            prefetch_factor=4 if num_workers > 0 else None
        )

        return dataloader

    def train_epoch(self, dataloader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        self.optimizer.zero_grad()

        pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
        for batch_idx, (src_feat, dst_feat, labels) in enumerate(pbar):
            src_feat = src_feat.to(self.device, non_blocking=True)
            dst_feat = dst_feat.to(self.device, non_blocking=True)
            labels = labels.to(self.device, non_blocking=True)

            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    predictions = self.model(src_feat, dst_feat).squeeze()

                loss = self.criterion(predictions.float(), labels)
                loss = loss / self.gradient_accumulation

                self.scaler.scale(loss).backward()

                if (batch_idx + 1) % self.gradient_accumulation == 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.gradient_clip_norm)

                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
            else:
                predictions = self.model(src_feat, dst_feat).squeeze()
                loss = self.criterion(predictions, labels)
                loss = loss / self.gradient_accumulation

                loss.backward()

                if (batch_idx + 1) % self.gradient_accumulation == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.gradient_clip_norm)
                    self.optimizer.step()
                    self.optimizer.zero_grad()

            total_loss += loss.item() * self.gradient_accumulation
            num_batches += 1

            current_lr = self.scheduler.get_last_lr()[0]
            pbar.set_postfix({
                'loss': f'{loss.item() * self.gradient_accumulation:.4f}',
                'lr': f'{current_lr:.6f}'
            })

        avg_loss = total_loss / num_batches
        return avg_loss

    def validate(self, dataloader):
        """éªŒè¯"""
        self.model.eval()

        all_preds = []
        all_labels = []

        with torch.no_grad():
            for src_feat, dst_feat, labels in tqdm(dataloader, desc="éªŒè¯", leave=False):
                src_feat = src_feat.to(self.device, non_blocking=True)
                dst_feat = dst_feat.to(self.device, non_blocking=True)

                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        predictions = self.model(src_feat, dst_feat).squeeze()
                else:
                    predictions = self.model(src_feat, dst_feat).squeeze()

                all_preds.extend(predictions.cpu().numpy())
                all_labels.extend(labels.numpy())

        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)

        # è®¡ç®—æŒ‡æ ‡
        auc = roc_auc_score(all_labels, all_preds)
        acc = accuracy_score(all_labels, (all_preds > 0.5).astype(int))
        prec = precision_score(all_labels, (all_preds > 0.5).astype(int), zero_division=0)
        rec = recall_score(all_labels, (all_preds > 0.5).astype(int), zero_division=0)
        f1 = f1_score(all_labels, (all_preds > 0.5).astype(int), zero_division=0)

        return auc, acc, prec, rec, f1

    def train(self, edges_train, labels_train, edges_val, labels_val, features):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ å¼€å§‹ç¨³å®šè®­ç»ƒ...")
        print(f"   â€¢ Epochs: {EPOCHS}")
        print(f"   â€¢ Batch Size: {BATCH_SIZE}")
        print(f"   â€¢ Learning Rate: {config.LEARNING_RATE} (with Warmup)")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self.create_dataloader(edges_train, labels_train, features, shuffle=True)
        val_loader = self.create_dataloader(edges_val, labels_val, features, shuffle=False)

        print(f"\nğŸ“Š æ•°æ®åŠ è½½å™¨:")
        print(f"   â€¢ è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
        print(f"   â€¢ éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")

        # è®­ç»ƒå¾ªç¯
        for epoch in range(1, EPOCHS + 1):
            epoch_start = time.time()

            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader, epoch)
            self.history['train_loss'].append(train_loss)

            # æ›´æ–°å­¦ä¹ ç‡ï¼ˆWarmup + Cosineï¼‰
            current_lr = self.scheduler.step()
            self.history['learning_rate'].append(current_lr)

            # éªŒè¯
            val_auc, val_acc, val_prec, val_rec, val_f1 = self.validate(val_loader)
            self.history['val_auc'].append(val_auc)
            self.history['val_acc'].append(val_acc)
            self.history['val_precision'].append(val_prec)
            self.history['val_recall'].append(val_rec)
            self.history['val_f1'].append(val_f1)

            epoch_time = time.time() - epoch_start

            # æ‰“å°ç»“æœ
            print(f"\n{'='*70}")
            print(f"Epoch {epoch}/{EPOCHS} [{epoch_time:.1f}s] | LR: {current_lr:.6f}")
            print(f"{'='*70}")
            print(f"  Loss: {train_loss:.4f}")
            print(f"  AUC:  {val_auc:.4f} | Acc: {val_acc:.4f} | Prec: {val_prec:.4f}")
            print(f"  Rec:  {val_rec:.4f} | F1:  {val_f1:.4f}")

            # Early Stoppingæ£€æŸ¥
            improvement = val_auc - self.best_auc

            if improvement > self.min_delta:
                self.best_auc = val_auc
                self.best_epoch = epoch
                self.no_improve_count = 0
                self.save_model("best_stable")
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (AUC={val_auc:.4f}) â­ [æå‡: +{improvement:.4f}]")
            else:
                self.no_improve_count += 1
                print(f"  â¸ï¸  æ— æ”¹å–„ ({self.no_improve_count}/{self.patience})")

                if self.no_improve_count >= self.patience:
                    print(f"\nâ¹ï¸  Early stopping: å·²{self.patience}è½®æ— æ”¹å–„ (min_delta={self.min_delta})")
                    break

        print(f"\n{'='*70}")
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"{'='*70}")
        print(f"   â€¢ æœ€ä½³AUC: {self.best_auc:.4f} (Epoch {self.best_epoch})")
        print(f"   â€¢ æ€»è®­ç»ƒè½®æ•°: {epoch}")

        return self.history

    def save_model(self, name="best_stable"):
        """ä¿å­˜æ¨¡å‹"""
        model_dir = os.path.join(current_dir, "models")
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, f"edge_predictor_{name}.pth")
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_auc': self.best_auc,
            'best_epoch': self.best_epoch,
            'config': {
                'learning_rate': config.LEARNING_RATE,
                'weight_decay': config.WEIGHT_DECAY,
                'hidden_dim': config.HIDDEN_DIM,
                'warmup_epochs': config.LR_WARMUP_EPOCHS
            }
        }, model_path)

    def save_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_file = os.path.join(current_dir, "results", "training_history_stable.json")
        os.makedirs(os.path.dirname(history_file), exist_ok=True)

        with open(history_file, 'w') as f:
            json.dump(self.history, f, indent=2)

        print(f"ğŸ’¾ è®­ç»ƒå†å²å·²ä¿å­˜: {history_file}")


def main():
    print("ğŸš€ æ­¥éª¤5: è®­ç»ƒè¾¹é¢„æµ‹å™¨ (ç¨³å®šç‰ˆ - Warmup + Lower LR)")
    print("=" * 70)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = StableEdgePredictorTrainer()

    # åŠ è½½æ•°æ®
    edges_train, labels_train, edges_val, labels_val, features = trainer.load_data()

    # è®­ç»ƒ
    history = trainer.train(edges_train, labels_train, edges_val, labels_val, features)

    # ä¿å­˜å†å²
    trainer.save_history()

    print("\n" + "=" * 70)
    print("âœ… æ­¥éª¤5å®Œæˆ: è¾¹é¢„æµ‹å™¨ç¨³å®šè®­ç»ƒå®Œæˆ")
    print(f"ğŸ“ æ¨¡å‹ä½ç½®: models/edge_predictor_best_stable.pth")
    print(f"ğŸ“Š æœ€ä½³AUC: {trainer.best_auc:.4f}")

    if trainer.best_auc > 0.65:
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸ! AUCæ˜¾è‘—é«˜äºéšæœºåŸºçº¿(0.50)")
    else:
        print("\nâš ï¸  AUCè¾ƒä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°æˆ–æ£€æŸ¥æ•°æ®")

    print("\nğŸ‘‰ ä¸‹ä¸€æ­¥: è¯„ä¼°ç¨³å®šæ¨¡å‹")
    print("   è¿è¡Œ: python 6_evaluate_model.py (éœ€è¦ä¿®æ”¹æ¨¡å‹è·¯å¾„)")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
