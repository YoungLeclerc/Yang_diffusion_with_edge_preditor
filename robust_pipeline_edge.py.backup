#!/usr/bin/env python3
"""
ä½¿ç”¨PPIè¾¹é¢„æµ‹å™¨çš„é²æ£’æ€§å¢å¼ºè®­ç»ƒ-æµ‹è¯•ç®¡é“

è¾¹é¢„æµ‹å™¨ä¼˜åŠ¿ (è¶…ç¨³å®šç‰ˆ v3.0):
  1. åŸºäºçœŸå®PPIçŸ¥è¯†: åœ¨1,858,944æ¡è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®ä¸Šè®­ç»ƒ (STRING v12.0)
  2. é«˜å‡†ç¡®åº¦: AUC=0.9300 (è®­ç»ƒ), 0.9297 (æµ‹è¯•)ï¼Œæ€§èƒ½ä¼˜å¼‚ï¼
  3. æ··åˆè¯„ä¼°æœºåˆ¶: è¾¹é¢„æµ‹åˆ†æ•° + ä½™å¼¦ç›¸ä¼¼åº¦ + æ¬§æ°è·ç¦»
  4. Top-Kä¿è¯: ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘æœ‰kä¸ªé‚»æ¥è¾¹
  5. æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›: å­¦åˆ°çš„PPIå…³ç³»å¯è¿ç§»åˆ°ä¸åŒè›‹ç™½è´¨
  6. è®­ç»ƒç¨³å®š: 66è½®æ— å´©æºƒï¼Œç›¸æ¯”ä¹‹å‰ç‰ˆæœ¬æå‡+3.12%
"""
import os
import time
import glob
import json
from tqdm import tqdm
import torch
from torch_geometric.data import Data
import numpy as np
from sklearn.model_selection import KFold
import warnings
warnings.filterwarnings('ignore')

from balanced_training_config import BalancedTrainingConfig
from improved_gnn_model import ImprovedBindingSiteGNN
from data_loader import ProteinDataset
from ddpm_diffusion_model import EnhancedDiffusionModel
from main import calculate_class_ratio
from gnn_model import set_seed
from edge_predictor_augmentation import (
    ImprovedEdgePredictor,
    robust_augment_dataset_with_edge_predictor,
    build_edges_with_edge_predictor
)


class RobustTrainingConfigWithEdgePredictor(BalancedTrainingConfig):
    """ä½¿ç”¨è¾¹é¢„æµ‹å™¨çš„é²æ£’è®­ç»ƒé…ç½®"""
    def __init__(self, target_ratio=0.9, experiment_name="default", use_edge_predictor=True):
        super().__init__()
        self.target_ratio = target_ratio
        self.experiment_name = experiment_name
        self.min_samples_per_protein = 5
        self.max_augment_ratio = 2.0

        # è´¨é‡æ§åˆ¶
        self.quality_threshold = 0.7
        self.diversity_threshold = 0.3

        # åŸŸé€‚åº”
        self.use_domain_adaptation = True
        self.domain_weight = 0.1

        # äº¤å‰éªŒè¯
        self.use_cross_validation = True
        self.cv_folds = 3

        # é›†æˆå­¦ä¹ 
        self.ensemble_size = 3

        # è¾¹é¢„æµ‹å™¨é…ç½®ï¼ˆğŸš€ GPUä¼˜åŒ–ç‰ˆ - å¹³è¡¡æ€§èƒ½ä¸è´¨é‡ï¼‰
        self.use_edge_predictor = use_edge_predictor
        self.edge_predictor_config = {
            'predictor_threshold': 0.8,   # ğŸš€ è¾ƒä¸¥æ ¼ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œè¾¹è´¨é‡ï¼‰
            'sim_threshold': 0.7,         # ğŸš€ é€‚ä¸­çš„ç›¸ä¼¼åº¦è¦æ±‚
            'dist_threshold': 1.2,        # ğŸš€ é€‚ä¸­çš„è·ç¦»é™åˆ¶
            'top_k': 5,                   # ä¿è¯åŸºæœ¬è¿é€šæ€§
            'connect_generated_nodes': True,   # âœ… ä¿ç•™ï¼ˆå¢å¼ºå›¾è¿é€šæ€§ï¼‰
            'use_topk_guarantee': True
        }

        # è¾“å‡ºæ§åˆ¶
        self.verbose_loading = False

        print(f"ğŸ¯ é²æ£’è®­ç»ƒé…ç½®:")
        print(f"  - ç›®æ ‡æ¯”ä¾‹: {self.target_ratio:.1%}")
        print(f"  - ä½¿ç”¨è¾¹é¢„æµ‹å™¨: {self.use_edge_predictor}")
        print(f"  - è¾¹é¢„æµ‹é˜ˆå€¼: {self.edge_predictor_config['predictor_threshold']}")
        print(f"  - Top-Kä¿è¯: {self.edge_predictor_config['top_k']}")
        print(f"Data directory: {self.data_dir}")
        print(f"Output directory: {self.output_dir}")


def load_or_train_edge_predictor(
    train_dataset,
    config,
    pretrained_path=None
):
    """
    åŠ è½½æˆ–è®­ç»ƒè¾¹é¢„æµ‹å™¨

    Args:
        train_dataset: è®­ç»ƒæ•°æ®é›†
        config: é…ç½®å¯¹è±¡
        pretrained_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„

    Returns:
        edge_predictor: è®­ç»ƒå¥½çš„æˆ–åŠ è½½çš„è¾¹é¢„æµ‹å™¨
    """
    print(f"ğŸ”— åˆå§‹åŒ–è¾¹é¢„æµ‹å™¨...")

    # è·å–ç‰¹å¾ç»´åº¦
    feature_dim = train_dataset[0].x.size(1)

    # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    if pretrained_path and os.path.exists(pretrained_path):
        print(f"âœ… åŠ è½½é¢„è®­ç»ƒè¾¹é¢„æµ‹å™¨: {pretrained_path}")
        try:
            # åŠ è½½checkpointæŸ¥çœ‹hidden_dim
            checkpoint = torch.load(pretrained_path, map_location='cpu', weights_only=False)
            # ä»checkpointæ¨æ–­hidden_dimï¼ˆå‡è®¾ä¿å­˜åœ¨state_dictä¸­ï¼‰
            # fc_transform.weight shape: [hidden_dim, input_dim]
            if 'fc_transform.weight' in checkpoint:
                hidden_dim = checkpoint['fc_transform.weight'].shape[0]
            else:
                # å¦‚æœæ˜¯å®Œæ•´çš„checkpointæ ¼å¼
                hidden_dim = checkpoint['model_state_dict']['fc_transform.weight'].shape[0] if 'model_state_dict' in checkpoint else 1024

            edge_predictor = ImprovedEdgePredictor(
                input_dim=feature_dim,
                hidden_dim=hidden_dim
            )

            # åŠ è½½æƒé‡
            if 'model_state_dict' in checkpoint:
                edge_predictor.load_state_dict(checkpoint['model_state_dict'])
            else:
                edge_predictor.load_state_dict(checkpoint)

            edge_predictor.eval()
            print(f"   æ¨¡å‹é…ç½®: input_dim={feature_dim}, hidden_dim={hidden_dim}")
            return edge_predictor
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
            print(f"   ä½¿ç”¨éšæœºåˆå§‹åŒ–")

    # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    edge_predictor = ImprovedEdgePredictor(
        input_dim=feature_dim,
        hidden_dim=358  # é»˜è®¤å€¼
    )

    # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–
    # (å®é™…åœºæ™¯ä¸­åº”è¯¥åœ¨æœ‰æ ‡æ³¨è¾¹çš„æ•°æ®ä¸Šè®­ç»ƒ)
    print(f"âš ï¸  æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    print(f"   å»ºè®®: åœ¨è›‹ç™½è´¨å›¾æ•°æ®ä¸Šé¢„å…ˆè®­ç»ƒè¾¹é¢„æµ‹å™¨")
    edge_predictor.to(config.device)
    edge_predictor.eval()

    return edge_predictor


def domain_adaptive_loss(predictions, targets, domain_weight=0.1):
    """åŸŸé€‚åº”æŸå¤±"""
    if predictions.dim() == 1:
        base_loss = torch.nn.functional.binary_cross_entropy_with_logits(predictions, targets.float())
    else:
        targets = targets.long()
        base_loss = torch.nn.functional.cross_entropy(predictions, targets)

    batch_size = predictions.size(0)
    if batch_size > 1:
        if predictions.dim() == 1:
            probs = torch.sigmoid(predictions)
            prob_var = torch.var(probs, dim=0)
        else:
            probs = torch.softmax(predictions, dim=1)
            prob_var = torch.var(probs, dim=0).mean()
        domain_loss = domain_weight * prob_var
    else:
        domain_loss = 0.0

    return base_loss + domain_loss


class RobustGNNModel(ImprovedBindingSiteGNN):
    """é²æ£’GNNæ¨¡å‹"""
    def __init__(self, input_dim, hidden_dim=128, dropout=0.3, use_focal_loss=True,
                 focal_alpha=0.75, focal_gamma=2.0, pos_weight=3.0, domain_weight=0.1):
        super().__init__(input_dim, hidden_dim, dropout, use_focal_loss,
                         focal_alpha, focal_gamma, pos_weight)
        self.domain_weight = domain_weight

    def train_with_domain_adaptation(self, train_data, val_data, epochs=100, lr=0.001,
                                    device='cuda', patience=10):
        """åŸŸé€‚åº”è®­ç»ƒ"""
        self.to(device)
        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)

        best_val_f1 = 0
        best_val_auc = 0
        patience_counter = 0

        for epoch in range(epochs):
            self.train()
            total_loss = 0

            for data in train_data:
                data = data.to(device)
                optimizer.zero_grad()

                out = self(data)
                loss = domain_adaptive_loss(out, data.y, self.domain_weight)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            if epoch % 10 == 0:
                val_metrics = self.evaluate(val_data, device)
                scheduler.step(val_metrics['f1'])

                if val_metrics['f1'] > best_val_f1:
                    best_val_f1 = val_metrics['f1']
                    best_val_auc = val_metrics['auc_pr']
                    patience_counter = 0
                else:
                    patience_counter += 1

                if patience_counter >= patience:
                    break

        return best_val_auc, best_val_f1


def cross_validation_training_with_edge_predictor(
    augmented_data,
    original_data,
    config,
    edge_predictor=None
):
    """äº¤å‰éªŒè¯è®­ç»ƒï¼ˆä½¿ç”¨è¾¹é¢„æµ‹å™¨æ„å»ºçš„å›¾ï¼‰"""
    print(f"\nğŸ”„ {config.cv_folds}æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒç­–ç•¥: ä½¿ç”¨è¾¹é¢„æµ‹å™¨æ„å»ºçš„å¢å¼ºå›¾")

    kf = KFold(n_splits=config.cv_folds, shuffle=True, random_state=config.seed)
    cv_results = []
    original_indices = list(range(len(original_data)))

    for fold, (_, val_idx) in enumerate(kf.split(original_indices)):
        print(f"\nğŸ“Š ç¬¬ {fold+1}/{config.cv_folds} æŠ˜")

        val_original_indices = set(val_idx)
        train_original = [original_data[i] for i in range(len(original_data))
                         if i not in val_original_indices]
        train_fold = augmented_data + train_original

        val_fold = [original_data[i] for i in val_idx]

        print(f"  ğŸ“ˆ è®­ç»ƒé›†å¤§å°: {len(train_fold)} (å¢å¼º: {len(augmented_data)}, "
              f"åŸå§‹: {len(train_original)})")
        print(f"  ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_fold)} (ä»…åŸå§‹æ•°æ®)")

        model = RobustGNNModel(
            input_dim=config.diffusion_input_dim,
            hidden_dim=config.gnn_hidden_dim,
            dropout=config.gnn_dropout,
            use_focal_loss=config.use_focal_loss,
            focal_alpha=config.focal_alpha,
            focal_gamma=config.focal_gamma,
            pos_weight=config.pos_weight,
            domain_weight=config.domain_weight
        )

        best_auc, best_f1 = model.train_with_domain_adaptation(
            train_fold, val_fold,
            epochs=config.gnn_epochs,
            lr=config.gnn_lr,
            device=config.device,
            patience=config.gnn_patience
        )

        cv_results.append({
            'model': model,
            'val_f1': best_f1,
            'val_auc': best_auc
        })

        print(f"  âœ… ç¬¬{fold+1}æŠ˜: F1={best_f1:.4f}, AUC-PR={best_auc:.4f}")

    return cv_results


def train_and_test_with_edge_predictor(
    train_file,
    test_files,
    config,
    edge_predictor=None,
    new_test_files=None
):
    """ä½¿ç”¨è¾¹é¢„æµ‹å™¨çš„è®­ç»ƒ-æµ‹è¯•æµç¨‹"""
    train_name = os.path.splitext(os.path.basename(train_file))[0]
    print(f"\nğŸš€ å¼€å§‹é²æ£’è®­ç»ƒ-æµ‹è¯• (ä½¿ç”¨è¾¹é¢„æµ‹å™¨): {train_name}")
    print("="*60)

    ratio_str = f"{config.target_ratio:.2f}".replace(".", "")
    output_dir_name = f"{train_name}_robust_r{ratio_str}_edgepred_{config.experiment_name}"
    output_path = os.path.join(config.output_dir, output_dir_name)
    os.makedirs(output_path, exist_ok=True)

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š é˜¶æ®µ1: åŠ è½½è®­ç»ƒæ•°æ®...")
    train_dataset = load_dataset_quiet(train_file, config)

    if not train_dataset:
        print(f"âŒ æ•°æ®é›†ä¸ºç©º: {train_file}")
        return None

    print(f"âœ… åŠ è½½äº† {len(train_dataset)} ä¸ªè›‹ç™½è´¨")

    orig_ratio, orig_pos, orig_neg = calculate_class_ratio(train_dataset)
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {orig_pos:,} æ­£æ ·æœ¬, {orig_neg:,} è´Ÿæ ·æœ¬ (æ¯”ä¾‹: {orig_ratio:.3%})")

    # è®­ç»ƒæ‰©æ•£æ¨¡å‹
    print(f"\nğŸ§  é˜¶æ®µ2: è®­ç»ƒæ‰©æ•£æ¨¡å‹...")
    diffusion_model = EnhancedDiffusionModel(
        input_dim=config.diffusion_input_dim,
        T=config.diffusion_T,
        device=config.device
    )

    diffusion_start = time.time()
    diffusion_model.train_on_positive_samples(
        train_dataset,
        epochs=config.diffusion_epochs,
        batch_size=config.diffusion_batch_size
    )
    diffusion_time = time.time() - diffusion_start
    print(f"âœ… æ‰©æ•£æ¨¡å‹è®­ç»ƒå®Œæˆ: {diffusion_time:.1f}ç§’")

    # åŠ è½½æˆ–åˆå§‹åŒ–è¾¹é¢„æµ‹å™¨
    print(f"\nğŸ”— é˜¶æ®µ2.5: åˆå§‹åŒ–è¾¹é¢„æµ‹å™¨ (è¶…ç¨³å®šç‰ˆ)...")
    if edge_predictor is None:
        # ä½¿ç”¨è®­ç»ƒå¥½çš„PPIè¾¹é¢„æµ‹å™¨æ¨¡å‹ - è¶…ç¨³å®šç‰ˆ (AUC=0.9300)
        current_dir = os.path.dirname(os.path.abspath(__file__))
        ppi_model_path = os.path.join(current_dir, "models", "edge_predictor_best_ultra_stable.pth")

        print(f"ğŸ“ ä½¿ç”¨è¶…ç¨³å®šç‰ˆPPIæ¨¡å‹ (è®­ç»ƒAUC: 0.9300, æµ‹è¯•AUC: 0.9297)")

        edge_predictor = load_or_train_edge_predictor(
            train_dataset,
            config,
            pretrained_path=ppi_model_path
        )

        # ğŸš€ GPUä¼˜åŒ–ï¼šç¡®ä¿è¾¹é¢„æµ‹å™¨åœ¨GPUä¸Š
        edge_predictor = edge_predictor.to(config.device)
        edge_predictor.eval()
        print(f"âœ… è¾¹é¢„æµ‹å™¨å·²åŠ è½½åˆ°GPU: {config.device}")

    # ä½¿ç”¨è¾¹é¢„æµ‹å™¨è¿›è¡Œé²æ£’å¢å¼º
    print(f"\nğŸ›¡ï¸ é˜¶æ®µ3: ä½¿ç”¨è¾¹é¢„æµ‹å™¨çš„é²æ£’å¢å¼º...")
    augment_start = time.time()
    augmented_data, aug_stats = robust_augment_dataset_with_edge_predictor(
        train_dataset,
        diffusion_model,
        edge_predictor,
        config,
        predictor_config=config.edge_predictor_config
    )
    augment_time = time.time() - augment_start

    aug_ratio, aug_pos, aug_neg = calculate_class_ratio(augmented_data)
    print(f"âœ… å¢å¼ºå®Œæˆ: {aug_pos:,} æ­£æ ·æœ¬, {aug_neg:,} è´Ÿæ ·æœ¬ "
          f"(æ¯”ä¾‹: {aug_ratio:.3%}) - ç”¨æ—¶: {augment_time:.1f}ç§’")

    # äº¤å‰éªŒè¯è®­ç»ƒ
    print(f"\nğŸ”„ é˜¶æ®µ4: äº¤å‰éªŒè¯è®­ç»ƒ...")
    gnn_start = time.time()
    cv_results = cross_validation_training_with_edge_predictor(
        augmented_data,
        train_dataset,
        config,
        edge_predictor
    )
    gnn_time = time.time() - gnn_start

    best_cv_result = max(cv_results, key=lambda x: x['val_f1'])
    best_model = best_cv_result['model']

    avg_f1 = np.mean([r['val_f1'] for r in cv_results])
    avg_auc = np.mean([r['val_auc'] for r in cv_results])

    print(f"âœ… äº¤å‰éªŒè¯å®Œæˆ: å¹³å‡F1={avg_f1:.4f}, å¹³å‡AUC-PR={avg_auc:.4f} - "
          f"ç”¨æ—¶: {gnn_time:.1f}ç§’")

    # ä¿å­˜æ¨¡å‹
    model_save_path = os.path.join(output_path, "robust_gnn_model.pt")
    torch.save(best_model.state_dict(), model_save_path)
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹ä¿å­˜è‡³: {model_save_path}")

    # æµ‹è¯•
    print(f"\nğŸ” é˜¶æ®µ5: æ¨¡å‹æµ‹è¯•")
    print("="*60)

    test_results = {}

    print(f"\n{'='*80}")
    print(f"ğŸ“Š åŸå§‹æµ‹è¯•é›† (DNAç³»åˆ—)")
    print(f"{'='*80}")

    for test_file in test_files:
        test_name = os.path.splitext(os.path.basename(test_file))[0]
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®é›†: {test_name}")

        try:
            test_dataset = load_dataset_quiet(test_file, config)
            print(f"âœ… åŠ è½½äº† {len(test_dataset)} ä¸ªè›‹ç™½è´¨")

            start_time = time.time()
            metrics = best_model.evaluate(test_dataset, device=config.device)
            eval_time = time.time() - start_time

            print(f"ğŸ“ˆ é²æ£’æµ‹è¯•ç»“æœ ({eval_time:.2f}s):")
            print(f"  ğŸ¯ F1 Score:         {metrics['f1']:.4f}")
            print(f"  ğŸ¯ MCC:              {metrics['mcc']:.4f}")
            print(f"  ğŸ¯ Accuracy:         {metrics['accuracy']:.4f}")
            print(f"  ğŸ¯ AUC-PR:           {metrics['auc_pr']:.4f}")
            print(f"  ğŸ¯ AUC-ROC:          {metrics['auc_roc']:.4f}")

            test_results[test_name] = metrics

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")

    # ä¿å­˜ç»“æœ
    total_time = time.time() - diffusion_start

    full_results = {
        "model_name": train_name + "_robust_edgepred",
        "model_type": "Robust GNN with Edge Predictor",
        "training_info": {
            "original_positive": orig_pos,
            "original_negative": orig_neg,
            "original_ratio": orig_ratio,
            "augmented_positive": aug_pos,
            "augmented_negative": aug_neg,
            "augmented_ratio": aug_ratio,
            "target_ratio": config.target_ratio,
            "cv_avg_f1": avg_f1,
            "cv_avg_auc": avg_auc,
            "diffusion_time": diffusion_time,
            "augment_time": augment_time,
            "gnn_time": gnn_time,
            "total_time": total_time
        },
        "edge_predictor_config": config.edge_predictor_config,
        "test_results": test_results
    }

    with open(os.path.join(output_path, "robust_results_edgepred.json"), 'w') as f:
        json.dump(full_results, f, indent=2, default=str)

    print(f"\nâœ… è®­ç»ƒ-æµ‹è¯•å®Œæˆ!")
    print(f"â±ï¸ æ€»ç”¨æ—¶: {total_time//60:.0f}m {total_time%60:.0f}s")

    return full_results


def load_dataset_quiet(dataset_file, config):
    """é™é»˜åŠ è½½æ•°æ®é›†"""
    import shutil
    import contextlib

    temp_data_dir = os.path.join(config.data_dir, "temp")
    os.makedirs(temp_data_dir, exist_ok=True)

    temp_file = os.path.join(temp_data_dir, os.path.basename(dataset_file))
    shutil.copy2(dataset_file, temp_file)

    try:
        if not config.verbose_loading:
            with open(os.devnull, 'w') as devnull:
                with contextlib.redirect_stdout(devnull):
                    dataset_loader = ProteinDataset(temp_data_dir, device=config.device)
                    dataset = dataset_loader.proteins
        else:
            dataset_loader = ProteinDataset(temp_data_dir, device=config.device)
            dataset = dataset_loader.proteins
    finally:
        if os.path.exists(temp_data_dir):
            shutil.rmtree(temp_data_dir)

    return dataset


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›¡ï¸ ä½¿ç”¨è¾¹é¢„æµ‹å™¨çš„é²æ£’ç®¡é“å¯åŠ¨")
    print("="*80)
    print("æ”¹è¿›ç­–ç•¥: è¾¹é¢„æµ‹ + è´¨é‡æ§åˆ¶ + åŸŸé€‚åº” + äº¤å‰éªŒè¯")
    print("="*80)

    config = RobustTrainingConfigWithEdgePredictor(use_edge_predictor=True)
    set_seed(config.seed)

    print(f"\nğŸ›¡ï¸ é…ç½®:")
    print(f"  - ä½¿ç”¨è¾¹é¢„æµ‹å™¨: {config.use_edge_predictor}")
    print(f"  - è¾¹é¢„æµ‹é˜ˆå€¼: {config.edge_predictor_config['predictor_threshold']}")

    train_files = sorted(glob.glob(os.path.join(config.data_dir, "*Train*.txt")))
    test_files = sorted(glob.glob(os.path.join(config.data_dir, "*Test*.txt")))

    original_test_files = [f for f in test_files if 'DNA-' in os.path.basename(f)]
    new_test_files = sorted(glob.glob(os.path.join(config.data_dir, "PDNA-*-test.txt")))

    print(f"\nğŸ” æ‰¾åˆ° {len(train_files)} ä¸ªè®­ç»ƒæ–‡ä»¶")
    print(f"   - åŸå§‹æµ‹è¯•é›† (DNAç³»åˆ—): {len(original_test_files)} ä¸ª")

    all_results = {}
    total_start = time.time()

    for train_file in train_files:
        try:
            result = train_and_test_with_edge_predictor(
                train_file,
                original_test_files,
                config
            )
            if result:
                all_results[os.path.splitext(os.path.basename(train_file))[0]] = result
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ {train_file}: {str(e)}")

    total_pipeline_time = time.time() - total_start

    if all_results:
        results_file = os.path.join(config.output_dir, "robust_pipeline_edgepred_results.json")
        with open(results_file, 'w') as f:
            json.dump(all_results, f, indent=2, default=str)

        print(f"\nğŸ‰ é²æ£’ç®¡é“æ‰§è¡Œå®Œæˆ!")
        print(f"â±ï¸ æ€»æ—¶é—´: {total_pipeline_time//3600:.0f}h {(total_pipeline_time%3600)//60:.0f}m")
        print(f"ğŸ“Š è¯¦ç»†ç»“æœ: {results_file}")


if __name__ == "__main__":
    main()
