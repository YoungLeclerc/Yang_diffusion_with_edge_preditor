#!/usr/bin/env python3
"""
æ­¥éª¤6: è¯„ä¼°è¾¹é¢„æµ‹å™¨æ¨¡å‹
åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
"""
import os
import sys
import json
import numpy as np
import torch
import matplotlib.pyplot as plt
from sklearn.metrics import (
    roc_auc_score, accuracy_score, precision_score,
    recall_score, f1_score, roc_curve, confusion_matrix,
    precision_recall_curve, average_precision_score
)
from torch.utils.data import TensorDataset, DataLoader
from tqdm import tqdm

# å¯¼å…¥è¾¹é¢„æµ‹å™¨æ¨¡å‹
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, parent_dir)
from edge_predictor_augmentation import ImprovedEdgePredictor

# å¯¼å…¥é…ç½®
current_dir = os.path.dirname(os.path.abspath(__file__))
import importlib.util
config_path = os.path.join(current_dir, "ppi_config.py")
spec = importlib.util.spec_from_file_location("ppi_config", config_path)
config = importlib.util.module_from_spec(spec)
spec.loader.exec_module(config)

FEATURE_DIM = config.FEATURE_DIM
DEVICE = config.DEVICE
BATCH_SIZE = config.BATCH_SIZE
PPI_PROCESSED_DIR = config.PPI_PROCESSED_DIR


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self, model_path):
        self.device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½æ¨¡å‹ï¼ˆhidden_diméœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        hidden_dim = getattr(config, 'HIDDEN_DIM', 1024)
        self.model = ImprovedEdgePredictor(input_dim=FEATURE_DIM, hidden_dim=hidden_dim).to(self.device)

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()

        self.best_auc = checkpoint.get('best_auc', 0.0)
        self.best_epoch = checkpoint.get('best_epoch', 0)

        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {model_path}")
        print(f"   â€¢ è®­ç»ƒæ—¶æœ€ä½³AUC: {self.best_auc:.4f} (Epoch {self.best_epoch})")

    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")

        # åŠ è½½æµ‹è¯•è¾¹å’Œæ ‡ç­¾
        edges_test = np.load(os.path.join(PPI_PROCESSED_DIR, "edges_test.npy"))
        labels_test = np.load(os.path.join(PPI_PROCESSED_DIR, "labels_test.npy"))

        # åŠ è½½ç‰¹å¾
        features = np.load(os.path.join(PPI_PROCESSED_DIR, "features.npy"))

        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   â€¢ æµ‹è¯•è¾¹: {len(edges_test):,}")
        print(f"     â””â”€ æ­£æ ·æœ¬: {(labels_test==1).sum():,}, è´Ÿæ ·æœ¬: {(labels_test==0).sum():,}")
        print(f"   â€¢ ç‰¹å¾ç»´åº¦: {features.shape}")

        return edges_test, labels_test, features

    def create_dataloader(self, edges, labels, features):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        src_feats = torch.tensor(features[edges[:, 0]], dtype=torch.float32)
        dst_feats = torch.tensor(features[edges[:, 1]], dtype=torch.float32)
        labels_tensor = torch.tensor(labels, dtype=torch.float32)

        dataset = TensorDataset(src_feats, dst_feats, labels_tensor)
        dataloader = DataLoader(
            dataset,
            batch_size=BATCH_SIZE,
            shuffle=False,
            num_workers=8,
            pin_memory=True
        )

        return dataloader

    def predict(self, dataloader):
        """åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹"""
        print("\nğŸ”® è¿›è¡Œé¢„æµ‹...")

        all_preds = []
        all_labels = []

        with torch.no_grad():
            for src_feat, dst_feat, labels in tqdm(dataloader, desc="é¢„æµ‹"):
                src_feat = src_feat.to(self.device)
                dst_feat = dst_feat.to(self.device)

                predictions = self.model(src_feat, dst_feat).squeeze()  # [batch, 1] -> [batch]

                all_preds.extend(predictions.cpu().numpy())
                all_labels.extend(labels.numpy())

        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)

        return all_preds, all_labels

    def calculate_metrics(self, preds, labels, threshold=0.5):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        print("\nğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")

        # äºŒåˆ†ç±»é¢„æµ‹
        binary_preds = (preds > threshold).astype(int)

        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'auc': roc_auc_score(labels, preds),
            'accuracy': accuracy_score(labels, binary_preds),
            'precision': precision_score(labels, binary_preds, zero_division=0),
            'recall': recall_score(labels, binary_preds, zero_division=0),
            'f1': f1_score(labels, binary_preds, zero_division=0),
            'average_precision': average_precision_score(labels, preds),
            'threshold': threshold
        }

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(labels, binary_preds)
        tn, fp, fn, tp = cm.ravel()

        metrics['confusion_matrix'] = {
            'true_negative': int(tn),
            'false_positive': int(fp),
            'false_negative': int(fn),
            'true_positive': int(tp)
        }

        # ç‰¹å¼‚æ€§å’Œæ•æ„Ÿæ€§
        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0
        metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0

        return metrics

    def plot_roc_curve(self, preds, labels, save_path):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        fpr, tpr, thresholds = roc_curve(labels, preds)
        auc = roc_auc_score(labels, preds)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.4f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random baseline')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)
        plt.legend(loc="lower right", fontsize=10)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… ROCæ›²çº¿å·²ä¿å­˜: {save_path}")

    def plot_precision_recall_curve(self, preds, labels, save_path):
        """ç»˜åˆ¶Precision-Recallæ›²çº¿"""
        precision, recall, thresholds = precision_recall_curve(labels, preds)
        ap = average_precision_score(labels, preds)

        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {ap:.4f})')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall', fontsize=12)
        plt.ylabel('Precision', fontsize=12)
        plt.title('Precision-Recall Curve', fontsize=14)
        plt.legend(loc="lower left", fontsize=10)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… PRæ›²çº¿å·²ä¿å­˜: {save_path}")

    def plot_confusion_matrix(self, cm_dict, save_path):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = np.array([
            [cm_dict['true_negative'], cm_dict['false_positive']],
            [cm_dict['false_negative'], cm_dict['true_positive']]
        ])

        plt.figure(figsize=(8, 6))
        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
        plt.title('Confusion Matrix', fontsize=14)
        plt.colorbar()

        classes = ['Negative', 'Positive']
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, fontsize=12)
        plt.yticks(tick_marks, classes, fontsize=12)

        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        thresh = cm.max() / 2.
        for i in range(2):
            for j in range(2):
                plt.text(j, i, format(cm[i, j], 'd'),
                        ha="center", va="center",
                        color="white" if cm[i, j] > thresh else "black",
                        fontsize=14)

        plt.ylabel('True label', fontsize=12)
        plt.xlabel('Predicted label', fontsize=12)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")

    def save_evaluation_report(self, metrics, save_path):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("è¾¹é¢„æµ‹å™¨æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 70 + "\n\n")

            f.write(f"è®­ç»ƒæ—¶æœ€ä½³AUC: {self.best_auc:.4f} (Epoch {self.best_epoch})\n\n")

            f.write("æµ‹è¯•é›†æ€§èƒ½:\n")
            f.write("-" * 70 + "\n")
            f.write(f"  AUC-ROC:           {metrics['auc']:.4f}\n")
            f.write(f"  Average Precision: {metrics['average_precision']:.4f}\n")
            f.write(f"  Accuracy:          {metrics['accuracy']:.4f}\n")
            f.write(f"  Precision:         {metrics['precision']:.4f}\n")
            f.write(f"  Recall:            {metrics['recall']:.4f}\n")
            f.write(f"  F1 Score:          {metrics['f1']:.4f}\n")
            f.write(f"  Sensitivity:       {metrics['sensitivity']:.4f}\n")
            f.write(f"  Specificity:       {metrics['specificity']:.4f}\n")
            f.write(f"  Decision Threshold: {metrics['threshold']:.2f}\n\n")

            f.write("æ··æ·†çŸ©é˜µ:\n")
            f.write("-" * 70 + "\n")
            cm = metrics['confusion_matrix']
            f.write(f"  True Negative:  {cm['true_negative']:,}\n")
            f.write(f"  False Positive: {cm['false_positive']:,}\n")
            f.write(f"  False Negative: {cm['false_negative']:,}\n")
            f.write(f"  True Positive:  {cm['true_positive']:,}\n\n")

            f.write("æ€§èƒ½è¯„ä¼°:\n")
            f.write("-" * 70 + "\n")
            if metrics['auc'] > 0.65:
                f.write("  âœ… AUCæ˜¾è‘—é«˜äºéšæœºåŸºçº¿(0.50)ï¼Œæ¨¡å‹å­¦ä¹ åˆ°äº†æœ‰æ•ˆçš„PPIæ¨¡å¼\n")
            else:
                f.write("  âš ï¸  AUCè¾ƒä½ï¼Œå¯èƒ½éœ€è¦:\n")
                f.write("     - è°ƒæ•´è¶…å‚æ•°\n")
                f.write("     - å¢åŠ è®­ç»ƒæ•°æ®\n")
                f.write("     - ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹æ¶æ„\n")

            f.write("\n" + "=" * 70 + "\n")

        print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {save_path}")


def main():
    print("ğŸ“Š æ­¥éª¤6: è¯„ä¼°è¾¹é¢„æµ‹å™¨æ¨¡å‹ (è¶…ç¨³å®šç‰ˆ)")
    print("=" * 70)

    # æ¨¡å‹è·¯å¾„ - ä½¿ç”¨è¶…ç¨³å®šç‰ˆ
    model_path = os.path.join(current_dir, "models", "edge_predictor_best_ultra_stable.pth")

    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œ: python 5_train_edge_predictor_ultra_stable.py")
        return False

    print(f"ğŸ“ è¯„ä¼°æ¨¡å‹: edge_predictor_best_ultra_stable.pth")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(model_path)

    # åŠ è½½æµ‹è¯•æ•°æ®
    edges_test, labels_test, features = evaluator.load_test_data()

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = evaluator.create_dataloader(edges_test, labels_test, features)

    # è¿›è¡Œé¢„æµ‹
    preds, labels = evaluator.predict(test_loader)

    # è®¡ç®—æŒ‡æ ‡
    metrics = evaluator.calculate_metrics(preds, labels)

    # æ‰“å°ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ")
    print("=" * 70)
    print(f"  AUC-ROC:           {metrics['auc']:.4f}")
    print(f"  Average Precision: {metrics['average_precision']:.4f}")
    print(f"  Accuracy:          {metrics['accuracy']:.4f}")
    print(f"  Precision:         {metrics['precision']:.4f}")
    print(f"  Recall:            {metrics['recall']:.4f}")
    print(f"  F1 Score:          {metrics['f1']:.4f}")
    print(f"  Sensitivity:       {metrics['sensitivity']:.4f}")
    print(f"  Specificity:       {metrics['specificity']:.4f}")

    # åˆ›å»ºç»“æœç›®å½• - è¶…ç¨³å®šç‰ˆä¸“ç”¨ç›®å½•
    results_dir = os.path.join(current_dir, "results_ultra_stable")
    os.makedirs(results_dir, exist_ok=True)

    print(f"\nğŸ’¾ ç»“æœå°†ä¿å­˜åˆ°: {results_dir}")

    # ç»˜åˆ¶å¯è§†åŒ–
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–...")
    evaluator.plot_roc_curve(
        preds, labels,
        os.path.join(results_dir, "roc_curve_ultra_stable.png")
    )
    evaluator.plot_precision_recall_curve(
        preds, labels,
        os.path.join(results_dir, "precision_recall_curve_ultra_stable.png")
    )
    evaluator.plot_confusion_matrix(
        metrics['confusion_matrix'],
        os.path.join(results_dir, "confusion_matrix_ultra_stable.png")
    )

    # ä¿å­˜æŒ‡æ ‡
    metrics_file = os.path.join(results_dir, "test_metrics_ultra_stable.json")
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"âœ… æŒ‡æ ‡å·²ä¿å­˜: {metrics_file}")

    # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    report_file = os.path.join(results_dir, "evaluation_report_ultra_stable.txt")
    evaluator.save_evaluation_report(metrics, report_file)

    print("\n" + "=" * 70)
    print("âœ… æ­¥éª¤6å®Œæˆ: è¶…ç¨³å®šç‰ˆæ¨¡å‹è¯„ä¼°å®Œæˆ")
    print(f"ğŸ“ ç»“æœä½ç½®: {results_dir}/")
    print(f"   â€¢ ROCæ›²çº¿: roc_curve_ultra_stable.png")
    print(f"   â€¢ PRæ›²çº¿: precision_recall_curve_ultra_stable.png")
    print(f"   â€¢ æ··æ·†çŸ©é˜µ: confusion_matrix_ultra_stable.png")
    print(f"   â€¢ è¯„ä¼°æŠ¥å‘Š: evaluation_report_ultra_stable.txt")

    if metrics['auc'] > 0.65:
        print("\nğŸ‰ æ¨¡å‹æ€§èƒ½ä¼˜ç§€! AUCæ˜¾è‘—é«˜äºéšæœºåŸºçº¿")
    else:
        print("\nâš ï¸  æ¨¡å‹æ€§èƒ½è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹æˆ–è°ƒæ•´è¶…å‚æ•°")

    print("\nğŸ‘‰ ä¸‹ä¸€æ­¥: é›†æˆåˆ°Pipeline")
    print("   è¿è¡Œ: python robust_pipeline_edge.py")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
