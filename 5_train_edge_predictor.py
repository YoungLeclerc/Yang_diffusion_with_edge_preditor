#!/usr/bin/env python3
"""
æ­¥éª¤5: è®­ç»ƒè¾¹é¢„æµ‹å™¨ï¼ˆçœŸå®PPIæ•°æ® + çœŸå®ESM2ç‰¹å¾ï¼‰
"""
import os
import sys
import json
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm

# å¯¼å…¥è¾¹é¢„æµ‹å™¨æ¨¡å‹
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, parent_dir)
from edge_predictor_augmentation import ImprovedEdgePredictor

# å¯¼å…¥é…ç½®
current_dir = os.path.dirname(os.path.abspath(__file__))
import importlib.util
config_path = os.path.join(current_dir, "ppi_config.py")
spec = importlib.util.spec_from_file_location("ppi_config", config_path)
config = importlib.util.module_from_spec(spec)
spec.loader.exec_module(config)

FEATURE_DIM = config.FEATURE_DIM
DEVICE = config.DEVICE
EPOCHS = config.TRAIN_EPOCHS
BATCH_SIZE = config.BATCH_SIZE
PPI_PROCESSED_DIR = config.PPI_PROCESSED_DIR


class EdgePredictorTrainer:
    """è¾¹é¢„æµ‹å™¨è®­ç»ƒå™¨"""

    def __init__(self):
        self.device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨æ›´å¤§çš„éšè—å±‚ï¼‰
        hidden_dim = getattr(config, 'HIDDEN_DIM', 1024)
        self.model = ImprovedEdgePredictor(input_dim=FEATURE_DIM, hidden_dim=hidden_dim).to(self.device)

        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {trainable_params:,} / {total_params:,} (å¯è®­ç»ƒ/æ€»è®¡)")

        # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨AdamW + weight decayï¼‰
        lr = getattr(config, 'LEARNING_RATE', 0.001)
        weight_decay = getattr(config, 'WEIGHT_DECAY', 1e-5)
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=EPOCHS, eta_min=1e-6
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.BCELoss()

        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = getattr(config, 'USE_AMP', True)
        self.scaler = torch.cuda.amp.GradScaler() if self.use_amp else None
        if self.use_amp:
            print(f"âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")

        # æ¢¯åº¦ç´¯ç§¯
        self.gradient_accumulation = getattr(config, 'GRADIENT_ACCUMULATION', 1)

        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'val_auc': [],
            'val_acc': [],
            'val_precision': [],
            'val_recall': [],
            'val_f1': []
        }

        self.best_auc = 0.0
        self.best_epoch = 0

    def load_data(self):
        """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
        print("\nğŸ“Š åŠ è½½æ•°æ®...")

        # åŠ è½½è¾¹
        edges_train = np.load(os.path.join(PPI_PROCESSED_DIR, "edges_train.npy"))
        edges_val = np.load(os.path.join(PPI_PROCESSED_DIR, "edges_val.npy"))

        # åŠ è½½æ ‡ç­¾
        labels_train = np.load(os.path.join(PPI_PROCESSED_DIR, "labels_train.npy"))
        labels_val = np.load(os.path.join(PPI_PROCESSED_DIR, "labels_val.npy"))

        # åŠ è½½ç‰¹å¾
        features = np.load(os.path.join(PPI_PROCESSED_DIR, "features.npy"))

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   â€¢ è®­ç»ƒè¾¹: {len(edges_train):,}")
        print(f"     â””â”€ æ­£: {(labels_train==1).sum():,}, è´Ÿ: {(labels_train==0).sum():,}")
        print(f"   â€¢ éªŒè¯è¾¹: {len(edges_val):,}")
        print(f"     â””â”€ æ­£: {(labels_val==1).sum():,}, è´Ÿ: {(labels_val==0).sum():,}")
        print(f"   â€¢ ç‰¹å¾: {features.shape}")

        return edges_train, labels_train, edges_val, labels_val, features

    def create_dataloader(self, edges, labels, features, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        src_feats = torch.tensor(features[edges[:, 0]], dtype=torch.float32)
        dst_feats = torch.tensor(features[edges[:, 1]], dtype=torch.float32)
        labels_tensor = torch.tensor(labels, dtype=torch.float32)

        dataset = TensorDataset(src_feats, dst_feats, labels_tensor)

        # ä½¿ç”¨æ›´å¤šçš„workersä»¥å……åˆ†åˆ©ç”¨CPU
        num_workers = getattr(config, 'NUM_WORKERS', 16)

        dataloader = DataLoader(
            dataset,
            batch_size=BATCH_SIZE,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=True,
            persistent_workers=True if num_workers > 0 else False,
            prefetch_factor=4 if num_workers > 0 else None
        )

        return dataloader

    def train_epoch(self, dataloader):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒæ··åˆç²¾åº¦ + æ¢¯åº¦ç´¯ç§¯ï¼‰"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        self.optimizer.zero_grad()

        pbar = tqdm(dataloader, desc="è®­ç»ƒ")
        for batch_idx, (src_feat, dst_feat, labels) in enumerate(pbar):
            src_feat = src_feat.to(self.device, non_blocking=True)
            dst_feat = dst_feat.to(self.device, non_blocking=True)
            labels = labels.to(self.device, non_blocking=True)

            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    predictions = self.model(src_feat, dst_feat).squeeze()  # [batch, 1] -> [batch]

                # BCELossåœ¨autocastä¹‹å¤–è®¡ç®—ï¼ˆå› ä¸ºæ¨¡å‹å·²æœ‰Sigmoidï¼‰
                loss = self.criterion(predictions.float(), labels)
                loss = loss / self.gradient_accumulation  # æ¢¯åº¦ç´¯ç§¯å½’ä¸€åŒ–

                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()

                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % self.gradient_accumulation == 0:
                    # æ¢¯åº¦è£å‰ª
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
            else:
                # æ ‡å‡†è®­ç»ƒ
                predictions = self.model(src_feat, dst_feat).squeeze()  # [batch, 1] -> [batch]
                loss = self.criterion(predictions, labels)
                loss = loss / self.gradient_accumulation

                loss.backward()

                if (batch_idx + 1) % self.gradient_accumulation == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                    self.optimizer.zero_grad()

            total_loss += loss.item() * self.gradient_accumulation
            num_batches += 1

            pbar.set_postfix({
                'loss': f'{loss.item() * self.gradient_accumulation:.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.6f}'
            })

        avg_loss = total_loss / num_batches
        return avg_loss

    def validate(self, dataloader):
        """éªŒè¯ï¼ˆæ”¯æŒæ··åˆç²¾åº¦ï¼‰"""
        self.model.eval()

        all_preds = []
        all_labels = []

        with torch.no_grad():
            for src_feat, dst_feat, labels in tqdm(dataloader, desc="éªŒè¯", leave=False):
                src_feat = src_feat.to(self.device, non_blocking=True)
                dst_feat = dst_feat.to(self.device, non_blocking=True)

                # æ··åˆç²¾åº¦æ¨ç†
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        predictions = self.model(src_feat, dst_feat).squeeze()  # [batch, 1] -> [batch]
                else:
                    predictions = self.model(src_feat, dst_feat).squeeze()  # [batch, 1] -> [batch]

                all_preds.extend(predictions.cpu().numpy())
                all_labels.extend(labels.numpy())

        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)

        # è®¡ç®—æŒ‡æ ‡
        auc = roc_auc_score(all_labels, all_preds)
        acc = accuracy_score(all_labels, (all_preds > 0.5).astype(int))
        prec = precision_score(all_labels, (all_preds > 0.5).astype(int), zero_division=0)
        rec = recall_score(all_labels, (all_preds > 0.5).astype(int), zero_division=0)
        f1 = f1_score(all_labels, (all_preds > 0.5).astype(int), zero_division=0)

        return auc, acc, prec, rec, f1

    def train(self, edges_train, labels_train, edges_val, labels_val, features):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"   â€¢ Epochs: {EPOCHS}")
        print(f"   â€¢ Batch Size: {BATCH_SIZE}")
        print(f"   â€¢ Learning Rate: 0.001")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self.create_dataloader(edges_train, labels_train, features, shuffle=True)
        val_loader = self.create_dataloader(edges_val, labels_val, features, shuffle=False)

        print(f"\nğŸ“Š æ•°æ®åŠ è½½å™¨:")
        print(f"   â€¢ è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
        print(f"   â€¢ éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")

        # è®­ç»ƒå¾ªç¯
        for epoch in range(1, EPOCHS + 1):
            epoch_start = time.time()

            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)
            self.history['train_loss'].append(train_loss)

            # éªŒè¯
            val_auc, val_acc, val_prec, val_rec, val_f1 = self.validate(val_loader)
            self.history['val_auc'].append(val_auc)
            self.history['val_acc'].append(val_acc)
            self.history['val_precision'].append(val_prec)
            self.history['val_recall'].append(val_rec)
            self.history['val_f1'].append(val_f1)

            epoch_time = time.time() - epoch_start

            # æ‰“å°ç»“æœ
            print(f"\nEpoch {epoch}/{EPOCHS} [{epoch_time:.1f}s]")
            print(f"  Loss: {train_loss:.4f}")
            print(f"  AUC: {val_auc:.4f} | Acc: {val_acc:.4f} | Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | F1: {val_f1:.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_auc > self.best_auc:
                self.best_auc = val_auc
                self.best_epoch = epoch
                self.save_model("best")
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (AUC={val_auc:.4f}) â­")

            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()

            # Early stoppingï¼ˆå¢åŠ åˆ°15è½®ï¼Œå› ä¸ºè®­ç»ƒè½®æ•°å¢åŠ äº†ï¼‰
            if epoch - self.best_epoch > 15:
                print(f"\nâ¹ï¸  Early stopping (æ— æ”¹å–„å·²15è½®)")
                break

        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"   â€¢ æœ€ä½³AUC: {self.best_auc:.4f} (Epoch {self.best_epoch})")

        return self.history

    def save_model(self, name="best"):
        """ä¿å­˜æ¨¡å‹"""
        model_dir = os.path.join(current_dir, "models")
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, f"edge_predictor_{name}.pth")
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_auc': self.best_auc,
            'best_epoch': self.best_epoch,
        }, model_path)

    def save_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_file = os.path.join(current_dir, "results", "training_history.json")
        os.makedirs(os.path.dirname(history_file), exist_ok=True)

        with open(history_file, 'w') as f:
            json.dump(self.history, f, indent=2)

        print(f"ğŸ’¾ è®­ç»ƒå†å²å·²ä¿å­˜: {history_file}")


def main():
    print("ğŸš€ æ­¥éª¤5: è®­ç»ƒè¾¹é¢„æµ‹å™¨ (çœŸå®æ•°æ® + ESM2ç‰¹å¾)")
    print("=" * 70)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = EdgePredictorTrainer()

    # åŠ è½½æ•°æ®
    edges_train, labels_train, edges_val, labels_val, features = trainer.load_data()

    # è®­ç»ƒ
    history = trainer.train(edges_train, labels_train, edges_val, labels_val, features)

    # ä¿å­˜å†å²
    trainer.save_history()

    print("\n" + "=" * 70)
    print("âœ… æ­¥éª¤5å®Œæˆ: è¾¹é¢„æµ‹å™¨è®­ç»ƒå®Œæˆ")
    print(f"ğŸ“ æ¨¡å‹ä½ç½®: models/edge_predictor_best.pth")
    print(f"ğŸ“Š æœ€ä½³AUC: {trainer.best_auc:.4f}")

    if trainer.best_auc > 0.65:
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸ! AUCæ˜¾è‘—é«˜äºéšæœºåŸºçº¿(0.50)")
    else:
        print("\nâš ï¸  AUCè¾ƒä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°æˆ–æ£€æŸ¥æ•°æ®")

    print("\nğŸ‘‰ ä¸‹ä¸€æ­¥: è¯„ä¼°æ¨¡å‹")
    print("   è¿è¡Œ: python 6_evaluate_model.py")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
