#!/usr/bin/env python3
"""
æ­¥éª¤5: è®­ç»ƒè¾¹é¢„æµ‹å™¨ï¼ˆè¶…ç¨³å®šç‰ˆ - é˜²æ­¢è®­ç»ƒå´©æºƒï¼‰

å…³é”®æ”¹è¿›:
1. å­¦ä¹ ç‡é™ä½60% (0.0005 â†’ 0.0002)
2. ä½¿ç”¨ReduceLROnPlateauæ›¿ä»£Cosineé€€ç«
3. æ›´å¼ºçš„æ¢¯åº¦è£å‰ªå’ŒL2æ­£åˆ™åŒ–
4. æ ‡ç­¾å¹³æ»‘
5. æ¢¯åº¦ç´¯ç§¯
"""
import os
import sys
import json
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm

# å¯¼å…¥è¾¹é¢„æµ‹å™¨æ¨¡å‹
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, parent_dir)
from edge_predictor_augmentation import ImprovedEdgePredictor

# å¯¼å…¥è¶…ç¨³å®šé…ç½®
current_dir = os.path.dirname(os.path.abspath(__file__))
import importlib.util
config_path = os.path.join(current_dir, "ppi_config_ultra_stable.py")
spec = importlib.util.spec_from_file_location("ppi_config_ultra_stable", config_path)
config = importlib.util.module_from_spec(spec)
spec.loader.exec_module(config)

FEATURE_DIM = config.FEATURE_DIM
DEVICE = config.DEVICE
EPOCHS = config.TRAIN_EPOCHS
BATCH_SIZE = config.BATCH_SIZE
PPI_PROCESSED_DIR = config.PPI_PROCESSED_DIR


class LabelSmoothingBCELoss(nn.Module):
    """å¸¦æ ‡ç­¾å¹³æ»‘çš„BCEæŸå¤±"""

    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(self, pred, target):
        # æ ‡ç­¾å¹³æ»‘: 0 â†’ smoothing/2, 1 â†’ 1-smoothing/2
        target = target * (1 - self.smoothing) + self.smoothing / 2
        return F.binary_cross_entropy(pred, target)


class WarmupScheduler:
    """Warmupå­¦ä¹ ç‡è°ƒåº¦å™¨"""

    def __init__(self, optimizer, warmup_epochs, base_lr):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.base_lr = base_lr
        self.current_epoch = 0

    def step(self):
        """æ›´æ–°å­¦ä¹ ç‡ï¼ˆä»…åœ¨warmupæœŸé—´ï¼‰"""
        self.current_epoch += 1

        if self.current_epoch <= self.warmup_epochs:
            # Warmupé˜¶æ®µï¼šçº¿æ€§å¢é•¿
            lr = self.base_lr * self.current_epoch / self.warmup_epochs
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
            return lr
        else:
            # Warmupåè¿”å›Noneï¼Œç”±ä¸»è°ƒåº¦å™¨æ¥ç®¡
            return None

    def get_last_lr(self):
        """è¿”å›å½“å‰å­¦ä¹ ç‡"""
        return [group['lr'] for group in self.optimizer.param_groups]


class UltraStableEdgePredictorTrainer:
    """è¶…ç¨³å®šç‰ˆè¾¹é¢„æµ‹å™¨è®­ç»ƒå™¨"""

    def __init__(self):
        self.device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆå§‹åŒ–æ¨¡å‹
        hidden_dim = getattr(config, 'HIDDEN_DIM', 1024)
        self.model = ImprovedEdgePredictor(input_dim=FEATURE_DIM, hidden_dim=hidden_dim).to(self.device)

        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {trainable_params:,} / {total_params:,} (å¯è®­ç»ƒ/æ€»è®¡)")

        # ä¼˜åŒ–å™¨ï¼ˆè¶…ç¨³å®šç‰ˆå‚æ•°ï¼‰
        lr = getattr(config, 'LEARNING_RATE', 0.0002)
        weight_decay = getattr(config, 'WEIGHT_DECAY', 5e-4)
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )

        print(f"ğŸ“ˆ å­¦ä¹ ç‡é…ç½®:")
        print(f"   â€¢ åŸºç¡€å­¦ä¹ ç‡: {lr} (è¶…ä½ï¼Œé˜²æ­¢å´©æºƒ)")
        print(f"   â€¢ Weight Decay: {weight_decay} (å¼ºæ­£åˆ™åŒ–)")

        # Warmupè°ƒåº¦å™¨
        warmup_epochs = getattr(config, 'WARMUP_EPOCHS', 5)
        self.warmup_scheduler = WarmupScheduler(
            self.optimizer,
            warmup_epochs=warmup_epochs,
            base_lr=lr
        )
        print(f"   â€¢ Warmup: {warmup_epochs} epochs")

        # ä¸»è°ƒåº¦å™¨: ReduceLROnPlateau (æ›´ç¨³å®š)
        lr_patience = getattr(config, 'LR_PATIENCE', 3)
        lr_factor = getattr(config, 'LR_FACTOR', 0.5)
        min_lr = getattr(config, 'LR_MIN', 1e-7)
        self.plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',
            factor=lr_factor,
            patience=lr_patience,
            min_lr=min_lr
        )
        print(f"   â€¢ Plateauè°ƒåº¦: patience={lr_patience}, factor={lr_factor}, min_lr={min_lr}")

        # æŸå¤±å‡½æ•°ï¼ˆå¸¦æ ‡ç­¾å¹³æ»‘ï¼‰
        label_smoothing = getattr(config, 'USE_LABEL_SMOOTHING', 0.1)
        if label_smoothing > 0:
            self.criterion = LabelSmoothingBCELoss(smoothing=label_smoothing)
            print(f"ğŸ¯ æ ‡ç­¾å¹³æ»‘: {label_smoothing}")
        else:
            self.criterion = nn.BCELoss()

        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = getattr(config, 'USE_AMP', True)
        self.scaler = torch.cuda.amp.GradScaler() if self.use_amp else None
        if self.use_amp:
            print(f"âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")

        # æ¢¯åº¦è£å‰ª
        self.gradient_clip_norm = getattr(config, 'GRADIENT_CLIP_NORM', 0.3)
        print(f"âœ‚ï¸  æ¢¯åº¦è£å‰ª: {self.gradient_clip_norm} (ä¸¥æ ¼)")

        # æ¢¯åº¦ç´¯ç§¯
        self.gradient_accumulation = getattr(config, 'GRADIENT_ACCUMULATION', 2)
        print(f"ğŸ”„ æ¢¯åº¦ç´¯ç§¯: {self.gradient_accumulation} steps (æ¨¡æ‹Ÿbatch={BATCH_SIZE * self.gradient_accumulation})")

        # Early Stopping
        self.patience = getattr(config, 'EARLY_STOPPING_PATIENCE', 15)
        self.min_delta = getattr(config, 'MIN_DELTA', 0.0001)
        print(f"â¸ï¸  Early Stopping: patience={self.patience}, min_delta={self.min_delta}")

        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'val_auc': [],
            'val_acc': [],
            'val_precision': [],
            'val_recall': [],
            'val_f1': [],
            'learning_rate': []
        }

        self.best_auc = 0.0
        self.best_epoch = 0
        self.no_improve_count = 0

    def load_data(self):
        """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
        print("\nğŸ“Š åŠ è½½æ•°æ®...")

        # åŠ è½½è¾¹
        edges_train = np.load(os.path.join(PPI_PROCESSED_DIR, "edges_train.npy"))
        edges_val = np.load(os.path.join(PPI_PROCESSED_DIR, "edges_val.npy"))

        # åŠ è½½æ ‡ç­¾
        labels_train = np.load(os.path.join(PPI_PROCESSED_DIR, "labels_train.npy"))
        labels_val = np.load(os.path.join(PPI_PROCESSED_DIR, "labels_val.npy"))

        # åŠ è½½ç‰¹å¾
        features = np.load(os.path.join(PPI_PROCESSED_DIR, "features.npy"))

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   â€¢ è®­ç»ƒè¾¹: {len(edges_train):,}")
        print(f"     â””â”€ æ­£: {(labels_train==1).sum():,}, è´Ÿ: {(labels_train==0).sum():,}")
        print(f"   â€¢ éªŒè¯è¾¹: {len(edges_val):,}")
        print(f"     â””â”€ æ­£: {(labels_val==1).sum():,}, è´Ÿ: {(labels_val==0).sum():,}")
        print(f"   â€¢ ç‰¹å¾: {features.shape}")

        return edges_train, labels_train, edges_val, labels_val, features

    def create_dataloader(self, edges, labels, features, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        src_feats = torch.tensor(features[edges[:, 0]], dtype=torch.float32)
        dst_feats = torch.tensor(features[edges[:, 1]], dtype=torch.float32)
        labels_tensor = torch.tensor(labels, dtype=torch.float32)

        dataset = TensorDataset(src_feats, dst_feats, labels_tensor)

        num_workers = getattr(config, 'NUM_WORKERS', 16)

        dataloader = DataLoader(
            dataset,
            batch_size=BATCH_SIZE,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=True,
            persistent_workers=True if num_workers > 0 else False,
            prefetch_factor=4 if num_workers > 0 else None
        )

        return dataloader

    def train_epoch(self, dataloader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        self.optimizer.zero_grad()

        pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
        for batch_idx, (src_feat, dst_feat, labels) in enumerate(pbar):
            src_feat = src_feat.to(self.device, non_blocking=True)
            dst_feat = dst_feat.to(self.device, non_blocking=True)
            labels = labels.to(self.device, non_blocking=True)

            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    predictions = self.model(src_feat, dst_feat).squeeze()

                loss = self.criterion(predictions.float(), labels)
                loss = loss / self.gradient_accumulation

                self.scaler.scale(loss).backward()

                if (batch_idx + 1) % self.gradient_accumulation == 0:
                    # æ¢¯åº¦è£å‰ª
                    self.scaler.unscale_(self.optimizer)
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        max_norm=self.gradient_clip_norm
                    )

                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
            else:
                predictions = self.model(src_feat, dst_feat).squeeze()
                loss = self.criterion(predictions, labels)
                loss = loss / self.gradient_accumulation

                loss.backward()

                if (batch_idx + 1) % self.gradient_accumulation == 0:
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        max_norm=self.gradient_clip_norm
                    )
                    self.optimizer.step()
                    self.optimizer.zero_grad()

            total_loss += loss.item() * self.gradient_accumulation
            num_batches += 1

            current_lr = self.optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'loss': f'{loss.item() * self.gradient_accumulation:.4f}',
                'lr': f'{current_lr:.7f}'
            })

        avg_loss = total_loss / num_batches
        return avg_loss

    def validate(self, dataloader):
        """éªŒè¯"""
        self.model.eval()

        all_preds = []
        all_labels = []

        with torch.no_grad():
            for src_feat, dst_feat, labels in tqdm(dataloader, desc="éªŒè¯", leave=False):
                src_feat = src_feat.to(self.device, non_blocking=True)
                dst_feat = dst_feat.to(self.device, non_blocking=True)

                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        predictions = self.model(src_feat, dst_feat).squeeze()
                else:
                    predictions = self.model(src_feat, dst_feat).squeeze()

                all_preds.extend(predictions.cpu().numpy())
                all_labels.extend(labels.numpy())

        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)

        # è®¡ç®—æŒ‡æ ‡
        auc = roc_auc_score(all_labels, all_preds)
        acc = accuracy_score(all_labels, (all_preds > 0.5).astype(int))
        prec = precision_score(all_labels, (all_preds > 0.5).astype(int), zero_division=0)
        rec = recall_score(all_labels, (all_preds > 0.5).astype(int), zero_division=0)
        f1 = f1_score(all_labels, (all_preds > 0.5).astype(int), zero_division=0)

        return auc, acc, prec, rec, f1

    def train(self, edges_train, labels_train, edges_val, labels_val, features):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ å¼€å§‹è¶…ç¨³å®šè®­ç»ƒ...")
        print(f"   â€¢ Epochs: {EPOCHS}")
        print(f"   â€¢ Batch Size: {BATCH_SIZE} x {self.gradient_accumulation} (ç´¯ç§¯)")
        print(f"   â€¢ Learning Rate: {config.LEARNING_RATE} (è¶…ä½)")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self.create_dataloader(edges_train, labels_train, features, shuffle=True)
        val_loader = self.create_dataloader(edges_val, labels_val, features, shuffle=False)

        print(f"\nğŸ“Š æ•°æ®åŠ è½½å™¨:")
        print(f"   â€¢ è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
        print(f"   â€¢ éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")

        # è®­ç»ƒå¾ªç¯
        for epoch in range(1, EPOCHS + 1):
            epoch_start = time.time()

            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader, epoch)
            self.history['train_loss'].append(train_loss)

            # Warmupé˜¶æ®µæ›´æ–°å­¦ä¹ ç‡
            if epoch <= self.warmup_scheduler.warmup_epochs:
                current_lr = self.warmup_scheduler.step()
            else:
                current_lr = self.optimizer.param_groups[0]['lr']

            self.history['learning_rate'].append(current_lr)

            # éªŒè¯
            val_auc, val_acc, val_prec, val_rec, val_f1 = self.validate(val_loader)
            self.history['val_auc'].append(val_auc)
            self.history['val_acc'].append(val_acc)
            self.history['val_precision'].append(val_prec)
            self.history['val_recall'].append(val_rec)
            self.history['val_f1'].append(val_f1)

            # Warmupåä½¿ç”¨Plateauè°ƒåº¦å™¨
            if epoch > self.warmup_scheduler.warmup_epochs:
                self.plateau_scheduler.step(val_auc)

            epoch_time = time.time() - epoch_start

            # æ‰“å°ç»“æœ
            print(f"\n{'='*70}")
            print(f"Epoch {epoch}/{EPOCHS} [{epoch_time:.1f}s] | LR: {current_lr:.7f}")
            print(f"{'='*70}")
            print(f"  Loss: {train_loss:.4f}")
            print(f"  AUC:  {val_auc:.4f} | Acc: {val_acc:.4f} | Prec: {val_prec:.4f}")
            print(f"  Rec:  {val_rec:.4f} | F1:  {val_f1:.4f}")

            # Early Stoppingæ£€æŸ¥
            improvement = val_auc - self.best_auc

            if improvement > self.min_delta:
                self.best_auc = val_auc
                self.best_epoch = epoch
                self.no_improve_count = 0
                self.save_model("best_ultra_stable")
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (AUC={val_auc:.4f}) â­ [æå‡: +{improvement:.4f}]")
            else:
                self.no_improve_count += 1
                print(f"  â¸ï¸  æ— æ”¹å–„ ({self.no_improve_count}/{self.patience})")

                if self.no_improve_count >= self.patience:
                    print(f"\nâ¹ï¸  Early stopping: å·²{self.patience}è½®æ— æ”¹å–„ (min_delta={self.min_delta})")
                    break

        print(f"\n{'='*70}")
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"{'='*70}")
        print(f"   â€¢ æœ€ä½³AUC: {self.best_auc:.4f} (Epoch {self.best_epoch})")
        print(f"   â€¢ æ€»è®­ç»ƒè½®æ•°: {epoch}")

        return self.history

    def save_model(self, name="best_ultra_stable"):
        """ä¿å­˜æ¨¡å‹"""
        model_dir = os.path.join(current_dir, "models")
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, f"edge_predictor_{name}.pth")
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_auc': self.best_auc,
            'best_epoch': self.best_epoch,
            'config': {
                'learning_rate': config.LEARNING_RATE,
                'weight_decay': config.WEIGHT_DECAY,
                'hidden_dim': config.HIDDEN_DIM,
                'warmup_epochs': config.WARMUP_EPOCHS,
                'gradient_accumulation': config.GRADIENT_ACCUMULATION
            }
        }, model_path)

    def save_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_file = os.path.join(current_dir, "results", "training_history_ultra_stable.json")
        os.makedirs(os.path.dirname(history_file), exist_ok=True)

        with open(history_file, 'w') as f:
            json.dump(self.history, f, indent=2)

        print(f"ğŸ’¾ è®­ç»ƒå†å²å·²ä¿å­˜: {history_file}")


def main():
    print("ğŸš€ æ­¥éª¤5: è®­ç»ƒè¾¹é¢„æµ‹å™¨ (è¶…ç¨³å®šç‰ˆ - é˜²æ­¢å´©æºƒ)")
    print("=" * 70)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = UltraStableEdgePredictorTrainer()

    # åŠ è½½æ•°æ®
    edges_train, labels_train, edges_val, labels_val, features = trainer.load_data()

    # è®­ç»ƒ
    history = trainer.train(edges_train, labels_train, edges_val, labels_val, features)

    # ä¿å­˜å†å²
    trainer.save_history()

    print("\n" + "=" * 70)
    print("âœ… æ­¥éª¤5å®Œæˆ: è¾¹é¢„æµ‹å™¨è¶…ç¨³å®šè®­ç»ƒå®Œæˆ")
    print(f"ğŸ“ æ¨¡å‹ä½ç½®: models/edge_predictor_best_ultra_stable.pth")
    print(f"ğŸ“Š æœ€ä½³AUC: {trainer.best_auc:.4f}")

    if trainer.best_auc > 0.90:
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸ! AUC>0.90ï¼Œæ€§èƒ½ä¼˜å¼‚")
    elif trainer.best_auc > 0.65:
        print("\nâœ… è®­ç»ƒæˆåŠŸ! AUCæ˜¾è‘—é«˜äºéšæœºåŸºçº¿(0.50)")
    else:
        print("\nâš ï¸  AUCè¾ƒä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°æˆ–æ£€æŸ¥æ•°æ®")

    print("\nğŸ‘‰ ä¸‹ä¸€æ­¥: è¯„ä¼°è¶…ç¨³å®šæ¨¡å‹")
    print("   è¿è¡Œ: python 6_evaluate_model.py")
    print("   (éœ€è¦ä¿®æ”¹æ¨¡å‹è·¯å¾„ä¸º edge_predictor_best_ultra_stable.pth)")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
