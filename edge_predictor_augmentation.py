#!/usr/bin/env python3
"""
ä½¿ç”¨è¾¹é¢„æµ‹å™¨æ„å»ºå›¾çš„æ”¹è¿›æ•°æ®å¢å¼ºæ¨¡å—
ç‰¹ç‚¹:
  1. ä½¿ç”¨è®­ç»ƒå¥½çš„è¾¹é¢„æµ‹å™¨æ›¿ä»£KNNæ„å»ºè¾¹
  2. æ··åˆç­–ç•¥: è¾¹é¢„æµ‹åˆ†æ•° + ä½™å¼¦ç›¸ä¼¼åº¦ + è·ç¦»é˜ˆå€¼
  3. ä¿è¯æœ€å°‘è¿æ¥æ•° (Top-K)
  4. æ”¯æŒå¢å¼ºèŠ‚ç‚¹é—´çš„è¿æ¥å’Œä¸åŸå§‹èŠ‚ç‚¹çš„è¿æ¥
  5. ğŸ”§ å®Œæ•´çš„è¾¹ç•Œæ£€æŸ¥å’Œé”™è¯¯å¤„ç†
"""
import torch
import torch.nn as nn
import numpy as np
from torch_geometric.data import Data
from torch.nn.functional import cosine_similarity
from tqdm import tqdm
from sklearn.metrics import pairwise_distances


class ImprovedEdgePredictor(nn.Module):
    """æ”¹è¿›çš„è¾¹é¢„æµ‹å™¨ - æ”¯æŒæƒé‡åŠ è½½å’Œå¤šç§è¿æ¥æ¨¡å¼"""
    def __init__(self, input_dim, hidden_dim=358):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        self.fc_transform = nn.Linear(input_dim, hidden_dim)
        self.model = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )

    def forward(self, xi, xj):
        """
        Args:
            xi: æºèŠ‚ç‚¹ç‰¹å¾ (batch_size_i, input_dim) æˆ– (batch_size_i, batch_size_j, input_dim)
            xj: ç›®æ ‡èŠ‚ç‚¹ç‰¹å¾ (batch_size_j, input_dim)

        Returns:
            è¾¹å­˜åœ¨æ¦‚ç‡ (batch_size_i * batch_size_j, 1)
        """
        xi_transformed = self.fc_transform(xi)
        xj_transformed = self.fc_transform(xj)

        # å¤„ç†ä¸åŒå¤§å°çš„æ‰¹é‡
        if xi_transformed.dim() == 2 and xj_transformed.dim() == 2:
            if xi_transformed.size(0) != xj_transformed.size(0):
                # æ‰©å±•ä¸ºæ‰€æœ‰ç»„åˆ
                xi_ext = xi_transformed.unsqueeze(1).repeat(1, xj_transformed.size(0), 1)
                xi_ext = xi_ext.view(-1, xi_ext.size(-1))
                xj_ext = xj_transformed.unsqueeze(0).repeat(xi_transformed.size(0), 1, 1)
                xj_ext = xj_ext.view(-1, xj_ext.size(-1))
                x_pair = torch.cat([xi_ext, xj_ext], dim=-1)
            else:
                # é€è¡Œå¯¹åº”
                x_pair = torch.cat([xi_transformed, xj_transformed], dim=-1)
        else:
            x_pair = torch.cat([xi_transformed, xj_transformed], dim=-1)

        return self.model(x_pair)


def build_edges_with_edge_predictor(
    original_data,
    generated_x,
    edge_predictor,
    device,
    predictor_threshold=0.5,
    sim_threshold=0.6,
    dist_threshold=1.5,
    top_k=5,
    connect_generated_nodes=True,
    use_topk_guarantee=True,
    verbose=False
):
    """
    ä½¿ç”¨è¾¹é¢„æµ‹å™¨ä¸ºå¢å¼ºåçš„å›¾æ„å»ºè¾¹

    Args:
        original_data: åŸå§‹å›¾æ•°æ® (PyG Data object)
        generated_x: ç”Ÿæˆçš„æ–°èŠ‚ç‚¹ç‰¹å¾ (num_new_nodes, feature_dim)
        edge_predictor: è®­ç»ƒå¥½çš„è¾¹é¢„æµ‹å™¨æ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        predictor_threshold: è¾¹é¢„æµ‹å™¨æ¦‚ç‡é˜ˆå€¼
        sim_threshold: ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼
        dist_threshold: æ¬§æ°è·ç¦»é˜ˆå€¼
        top_k: ä¿è¯è‡³å°‘è¿æ¥çš„é‚»æ¥æ•°
        connect_generated_nodes: æ˜¯å¦è¿æ¥ç”ŸæˆèŠ‚ç‚¹é—´çš„è¾¹
        use_topk_guarantee: æ˜¯å¦ä½¿ç”¨Top-Kä¿è¯
        verbose: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯

    Returns:
        augmented_data: åŒ…å«æ–°èŠ‚ç‚¹å’Œæ–°è¾¹çš„å›¾æ•°æ®
    """
    # ğŸ”§ ä¿®å¤ï¼šå¤„ç†ç©ºæ•°æ®çš„æƒ…å†µ
    if original_data.x is None or generated_x is None:
        return original_data, {'num_new_edges': 0, 'total_edges': 0, 'edge_scores': []}

    if generated_x.size(0) == 0:
        return original_data, {'num_new_edges': 0, 'total_edges': original_data.edge_index.size(1), 'edge_scores': []}

    original_x = original_data.x.to(device)
    generated_x = generated_x.to(device)

    # ğŸ”§ ä¿®å¤ï¼šå¤„ç†ç©ºè¾¹åˆ—è¡¨çš„æƒ…å†µ
    if original_data.edge_index.numel() == 0:
        original_edges = []
    else:
        original_edges = original_data.edge_index.t().tolist()

    new_node_start_idx = original_x.size(0)
    num_new_nodes = generated_x.size(0)

    edge_predictor.eval()
    new_edges = []
    edge_scores = []  # è®°å½•è¾¹çš„é¢„æµ‹åˆ†æ•°

    if verbose:
        print(f"ğŸ”— å¼€å§‹æ„å»ºæ–°è¾¹...")
        print(f"   åŸå§‹èŠ‚ç‚¹æ•°: {original_x.size(0)}")
        print(f"   ç”ŸæˆèŠ‚ç‚¹æ•°: {num_new_nodes}")

    # ==========================================
    # ç¬¬1éƒ¨åˆ†: ç”ŸæˆèŠ‚ç‚¹ä¸åŸå§‹èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥
    # ==========================================
    with torch.no_grad():
        for i_new in range(num_new_nodes):
            x_new = generated_x[i_new].unsqueeze(0)  # (1, feature_dim)

            # æ‰©å±•x_newæ¥ä¸æ‰€æœ‰åŸå§‹èŠ‚ç‚¹è®¡ç®—
            x_new_expanded = x_new.repeat(original_x.size(0), 1)  # (num_orig, feature_dim)

            # 1. è¾¹é¢„æµ‹å™¨åˆ†æ•°
            pred_scores = edge_predictor(x_new_expanded, original_x).squeeze()  # (num_orig,)
            if pred_scores.dim() == 0:
                pred_scores = pred_scores.unsqueeze(0)

            # 2. ä½™å¼¦ç›¸ä¼¼åº¦
            cos_sim = cosine_similarity(x_new_expanded, original_x).squeeze()  # (num_orig,)
            if cos_sim.dim() == 0:
                cos_sim = cos_sim.unsqueeze(0)

            # 3. æ¬§æ°è·ç¦»
            dist = torch.norm(x_new_expanded - original_x, dim=1)  # (num_orig,)

            # é€‰æ‹©è¿æ¥æ–¹å¼
            if use_topk_guarantee:
                # æ··åˆç­–ç•¥: æ»¡è¶³æ¡ä»¶ OR Top-K
                condition_mask = (pred_scores > predictor_threshold) & \
                                (cos_sim > sim_threshold) & \
                                (dist < dist_threshold)
                selected_idx = torch.nonzero(condition_mask).squeeze().tolist()
                if isinstance(selected_idx, int):
                    selected_idx = [selected_idx]
                elif not isinstance(selected_idx, list):
                    selected_idx = selected_idx.tolist()

                # ä¿è¯Top-K
                topk_vals, topk_idx = torch.topk(
                    pred_scores,
                    k=min(top_k, pred_scores.size(0))
                )
                topk_idx = topk_idx.tolist()

                # åˆå¹¶: æ¡ä»¶æ»¡è¶³çš„èŠ‚ç‚¹ + Top-KèŠ‚ç‚¹
                final_idx = set(selected_idx) | set(topk_idx)
            else:
                # ä¸¥æ ¼æ¡ä»¶
                condition_mask = (pred_scores > predictor_threshold) & \
                                (cos_sim > sim_threshold) & \
                                (dist < dist_threshold)
                final_idx = torch.nonzero(condition_mask).squeeze().tolist()
                if isinstance(final_idx, int):
                    final_idx = [final_idx]
                elif not isinstance(final_idx, list):
                    final_idx = final_idx.tolist()

            # æ·»åŠ åŒå‘è¾¹
            new_idx = new_node_start_idx + i_new
            for idx in final_idx:
                if 0 <= idx < original_x.size(0):
                    idx = int(idx)
                    # è®°å½•åˆ†æ•°
                    try:
                        score = float(pred_scores[idx].cpu().numpy())
                        edge_scores.append(score)
                    except:
                        pass

                    # æ·»åŠ åŒå‘è¾¹
                    new_edges.append([idx, new_idx])
                    new_edges.append([new_idx, idx])

            if verbose and i_new % max(1, num_new_nodes // 10) == 0:
                print(f"   å·²å¤„ç† {i_new+1}/{num_new_nodes} ä¸ªç”ŸæˆèŠ‚ç‚¹, "
                      f"å½“å‰è¿æ¥æ•°: {len(final_idx)}")

    # ==========================================
    # ç¬¬2éƒ¨åˆ†: ç”ŸæˆèŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥ (å¯é€‰)
    # ==========================================
    if connect_generated_nodes and num_new_nodes > 1:
        if verbose:
            print(f"ğŸ”— æ„å»ºç”ŸæˆèŠ‚ç‚¹é—´çš„è¾¹...")

        with torch.no_grad():
            for i in range(num_new_nodes):
                for j in range(i + 1, num_new_nodes):
                    x_i = generated_x[i].unsqueeze(0)  # (1, feature_dim)
                    x_j = generated_x[j].unsqueeze(0)  # (1, feature_dim)

                    # è®¡ç®—è¿æ¥æŒ‡æ ‡
                    pred_score = edge_predictor(x_i, x_j).squeeze()  # scalar
                    cos_sim_val = cosine_similarity(x_i, x_j).squeeze()  # scalar
                    dist_val = torch.norm(x_i - x_j)  # scalar

                    # åˆ¤æ–­æ˜¯å¦è¿æ¥
                    if (pred_score > predictor_threshold and
                        cos_sim_val > sim_threshold and
                        dist_val < dist_threshold):

                        i_idx = new_node_start_idx + i
                        j_idx = new_node_start_idx + j

                        new_edges.append([i_idx, j_idx])
                        new_edges.append([j_idx, i_idx])
                        try:
                            edge_scores.append(float(pred_score.cpu().numpy()))
                        except:
                            pass

    # ==========================================
    # ç¬¬3éƒ¨åˆ†: ç»„åˆæ‰€æœ‰è¾¹å’ŒèŠ‚ç‚¹
    # ==========================================
    all_x = torch.cat([original_x.cpu(), generated_x.cpu()], dim=0)
    all_y = torch.cat([
        original_data.y.cpu(),
        torch.ones(num_new_nodes, dtype=torch.long)
    ], dim=0)

    # ğŸ”§ ä¿®å¤ï¼šéªŒè¯å’Œæ¸…ç†æ‰€æœ‰è¾¹çš„ç´¢å¼•
    all_edges = original_edges + new_edges
    valid_edges = []
    total_nodes = all_x.size(0)

    for edge in all_edges:
        if len(edge) >= 2:
            src, dst = int(edge[0]), int(edge[1])
            # æ£€æŸ¥è¾¹æ˜¯å¦è¶…å‡ºèŒƒå›´
            if 0 <= src < total_nodes and 0 <= dst < total_nodes:
                valid_edges.append([src, dst])
            else:
                # ä¸¢å¼ƒæ— æ•ˆè¾¹
                if verbose:
                    print(f"âš ï¸  åˆ é™¤æ— æ•ˆè¾¹: [{src}, {dst}] (æ€»èŠ‚ç‚¹æ•°: {total_nodes})")

    # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæ²¡æœ‰æœ‰æ•ˆè¾¹ï¼Œåˆ›å»ºè‡³å°‘ä¸€æ¡è‡ªå¾ªç¯è¾¹é¿å…ç©ºå›¾
    if len(valid_edges) == 0:
        valid_edges = [[0, 0]]

    edge_index = torch.tensor(valid_edges, dtype=torch.long).t().contiguous()

    # åˆ›å»ºå¢å¼ºåçš„å›¾
    augmented_data = Data(
        x=all_x,
        edge_index=edge_index,
        y=all_y
    )

    if verbose:
        print(f"âœ… è¾¹æ„å»ºå®Œæˆ!")
        print(f"   æ–°å¢è¾¹æ•°: {len(new_edges)}")
        print(f"   æœ‰æ•ˆè¾¹æ•°: {edge_index.size(1)}")
        if edge_scores:
            print(f"   è¾¹é¢„æµ‹åˆ†æ•°èŒƒå›´: [{np.min(edge_scores):.3f}, {np.max(edge_scores):.3f}]")
            print(f"   è¾¹é¢„æµ‹åˆ†æ•°å¹³å‡å€¼: {np.mean(edge_scores):.3f}")

    return augmented_data, {
        'num_new_edges': len(new_edges),
        'total_edges': edge_index.size(1),
        'edge_scores': edge_scores
    }


def robust_augment_dataset_with_edge_predictor(
    dataset,
    diffusion_model,
    edge_predictor,
    config,
    predictor_config=None
):
    """
    ä½¿ç”¨è¾¹é¢„æµ‹å™¨çš„é²æ£’æ•°æ®å¢å¼º

    Args:
        dataset: åŸå§‹æ•°æ®é›†
        diffusion_model: æ‰©æ•£æ¨¡å‹
        edge_predictor: è¾¹é¢„æµ‹å™¨
        config: è®­ç»ƒé…ç½®
        predictor_config: è¾¹é¢„æµ‹å™¨é…ç½® (dict)
            {
                'predictor_threshold': 0.5,
                'sim_threshold': 0.6,
                'dist_threshold': 1.5,
                'top_k': 5,
                'connect_generated_nodes': True,
                'use_topk_guarantee': True
            }

    Returns:
        augmented_data: å¢å¼ºåçš„æ•°æ®é›†
        stats: ç»Ÿè®¡ä¿¡æ¯
    """
    if predictor_config is None:
        predictor_config = {
            'predictor_threshold': 0.5,
            'sim_threshold': 0.6,
            'dist_threshold': 1.5,
            'top_k': 5,
            'connect_generated_nodes': True,
            'use_topk_guarantee': True
        }

    augmented_data = []
    quality_stats = []
    diversity_stats = []
    edge_stats = []

    print(f"ğŸ¯ ä½¿ç”¨è¾¹é¢„æµ‹å™¨çš„é²æ£’å¢å¼ºç­–ç•¥:")
    print(f"  - ç›®æ ‡æ¯”ä¾‹: {config.target_ratio:.1%}")
    print(f"  - è´¨é‡é˜ˆå€¼: {config.quality_threshold}")
    print(f"  - å¤šæ ·æ€§é˜ˆå€¼: {config.diversity_threshold}")
    print(f"  - è¾¹é¢„æµ‹é˜ˆå€¼: {predictor_config['predictor_threshold']}")
    print(f"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {predictor_config['sim_threshold']}")
    print(f"  - è·ç¦»é˜ˆå€¼: {predictor_config['dist_threshold']}")

    edge_predictor.to(config.device)
    edge_predictor.eval()

    for data in tqdm(dataset, desc="Robust augmenting with edge predictor"):
        try:
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†ç¼ºå¤±protein_contextçš„æƒ…å†µ
            if hasattr(data, 'protein_context'):
                protein_context = data.protein_context.to(config.device)
            else:
                protein_context = data.x.to(config.device)

            # æå–æ­£æ ·æœ¬ç”¨äºè´¨é‡è¯„ä¼°
            pos_mask = (data.y == 1)
            if pos_mask.sum() == 0:
                augmented_data.append(data)
                continue

            real_pos_samples = data.x[pos_mask].cpu().numpy()
            n_pos = pos_mask.sum().item()
            n_neg = (data.y == 0).sum().item()
            total_nodes = n_pos + n_neg

            # è®¡ç®—ç”Ÿæˆæ•°é‡
            target_pos = int(total_nodes * config.target_ratio)
            n_to_generate = max(config.min_samples_per_protein, target_pos - n_pos)
            n_to_generate = min(n_to_generate, int(n_pos * config.max_augment_ratio))

            if n_to_generate > 0:
                # ç”Ÿæˆå€™é€‰æ ·æœ¬
                candidate_samples = diffusion_model.generate_positive_sample(
                    protein_context,
                    num_samples=n_to_generate * 3,
                    verbose=config.verbose_loading
                )

                if candidate_samples is None or len(candidate_samples) == 0:
                    augmented_data.append(data)
                    continue

                # è´¨é‡æ§åˆ¶
                quality_samples, quality_score = calculate_sample_quality(
                    candidate_samples, real_pos_samples, config.quality_threshold
                )

                # å¤šæ ·æ€§æ§åˆ¶
                if len(quality_samples) > 0:
                    diverse_samples, diversity_score = calculate_sample_diversity(
                        quality_samples, config.diversity_threshold
                    )
                else:
                    diverse_samples, diversity_score = candidate_samples[:n_to_generate], 0.5

                final_samples = diverse_samples[:n_to_generate]

                quality_stats.append(quality_score)
                diversity_stats.append(diversity_score)

                if len(final_samples) > 0:
                    # ä½¿ç”¨è¾¹é¢„æµ‹å™¨æ„å»ºè¾¹
                    new_x = torch.tensor(final_samples, dtype=torch.float32)

                    augmented_graph, edge_info = build_edges_with_edge_predictor(
                        data,
                        new_x,
                        edge_predictor,
                        config.device,
                        **predictor_config,
                        verbose=config.verbose_loading
                    )

                    edge_stats.append(edge_info)

                    # é™åˆ¶å›¾å¤§å°
                    if len(augmented_graph.x) > config.max_nodes_per_graph:
                        pos_mask_new = (augmented_graph.y == 1)
                        neg_mask_new = (augmented_graph.y == 0)

                        pos_indices = torch.where(pos_mask_new)[0]
                        neg_indices = torch.where(neg_mask_new)[0]

                        max_neg = config.max_nodes_per_graph - len(pos_indices)
                        if max_neg > 0 and len(neg_indices) > max_neg:
                            keep_neg = neg_indices[torch.randperm(len(neg_indices))[:max_neg]]
                            keep_indices = torch.cat([pos_indices, keep_neg])
                        else:
                            keep_indices = torch.arange(len(augmented_graph.x))

                        augmented_graph.x = augmented_graph.x[keep_indices]
                        augmented_graph.y = augmented_graph.y[keep_indices]

                        # ğŸ”§ ä¿®å¤ï¼šæ›´æ–°è¾¹ç´¢å¼•æ—¶ä¹Ÿéœ€è¦åšè¾¹ç•Œæ£€æŸ¥
                        if augmented_graph.edge_index.numel() > 0:
                            mask = torch.isin(augmented_graph.edge_index[0], keep_indices) & \
                                   torch.isin(augmented_graph.edge_index[1], keep_indices)
                            augmented_graph.edge_index = augmented_graph.edge_index[:, mask]

                    # ä¿å­˜è›‹ç™½è´¨ä¸Šä¸‹æ–‡ä¿¡æ¯
                    if hasattr(data, 'protein_context'):
                        augmented_graph.protein_context = data.protein_context
                    if hasattr(data, 'name'):
                        augmented_graph.name = data.name + "_aug_edgepred"

                    augmented_data.append(augmented_graph)
                else:
                    augmented_data.append(data)
            else:
                augmented_data.append(data)

        except Exception as e:
            print(f"âš ï¸  Warning: Augmentation failed for {getattr(data, 'name', 'unknown')}: {e}")
            augmented_data.append(data)

    # ç»Ÿè®¡ä¿¡æ¯
    if quality_stats:
        avg_quality = np.mean(quality_stats)
        avg_diversity = np.mean(diversity_stats)
        print(f"âœ… å¢å¼ºè´¨é‡: å¹³å‡è´¨é‡={avg_quality:.3f}, å¹³å‡å¤šæ ·æ€§={avg_diversity:.3f}")

    if edge_stats:
        total_new_edges = sum([e['num_new_edges'] for e in edge_stats])
        avg_score = np.mean([np.mean(e['edge_scores']) for e in edge_stats if e['edge_scores']])
        print(f"âœ… è¾¹é¢„æµ‹ç»Ÿè®¡: æ€»æ–°å¢è¾¹æ•°={total_new_edges}, å¹³å‡è¾¹åˆ†æ•°={avg_score:.3f}")

    return augmented_data, {
        'quality_scores': quality_stats,
        'diversity_scores': diversity_stats,
        'edge_stats': edge_stats
    }


def calculate_sample_quality(generated_samples, real_samples, threshold=0.7):
    """è¯„ä¼°ç”Ÿæˆæ ·æœ¬è´¨é‡"""
    if len(generated_samples) == 0 or len(real_samples) == 0:
        return [], 0.0

    try:
        distances = pairwise_distances(generated_samples, real_samples, metric='euclidean')
        min_distances = np.min(distances, axis=1)

        max_dist = np.max(min_distances) + 1e-8
        quality_scores = 1.0 - (min_distances / max_dist)

        high_quality_mask = quality_scores >= threshold
        high_quality_samples = generated_samples[high_quality_mask]
        avg_quality = np.mean(quality_scores)

        return high_quality_samples, avg_quality
    except:
        return generated_samples, 0.5


def calculate_sample_diversity(samples, threshold=0.3):
    """è¯„ä¼°æ ·æœ¬å¤šæ ·æ€§"""
    if len(samples) <= 1:
        return samples, 1.0

    try:
        distances = pairwise_distances(samples, metric='euclidean')
        np.fill_diagonal(distances, np.inf)

        diverse_indices = []
        for i in range(len(samples)):
            min_dist = np.min(distances[i])
            if len(diverse_indices) == 0 or min_dist >= threshold:
                diverse_indices.append(i)

        diverse_samples = samples[diverse_indices]
        diversity_score = len(diverse_samples) / len(samples)

        return diverse_samples, diversity_score
    except:
        return samples, 1.0
